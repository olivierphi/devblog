{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Olivier Philippon's devblog \u00b6 After more than 20 years spent in web development without having ever taken notes of all the \"tech-wise\" stuff I have in my head, its time to ease my own cognitive overload and start taking notes These notes are mostly for myself, for future reference - but who knows, they might as well be useful to other people Blog posts: 2023-08-18 -- business_logic.py 2023-05-26 -- Let's try Kitty layouts 2023-05-17 -- TIL: Hashing data in the form of a UUID 2022-11-11 -- Using Alembic with SQLAlchemy 2 2022-08-22 -- Porting a Python web app to Go 2022-08-09 -- 'From scratch to online in production' in a single day, with Django - Part 3 2022-07-28 -- Making SQLite much faster in a local dev environment 2022-07-26 -- Triggering a GitHub Action from an external source 2022-07-22 -- 'From scratch to online in production' in a single day, with Django - Part 2 2022-07-19 -- 'From scratch to online in production' in a single day, with Django - Part 1 2022-07-12 -- Building a Markdown-based blog 2022-07-10 -- Choosing a tech stack for my card game platform","title":"Homepage"},{"location":"#olivier-philippons-devblog","text":"After more than 20 years spent in web development without having ever taken notes of all the \"tech-wise\" stuff I have in my head, its time to ease my own cognitive overload and start taking notes These notes are mostly for myself, for future reference - but who knows, they might as well be useful to other people Blog posts: 2023-08-18 -- business_logic.py 2023-05-26 -- Let's try Kitty layouts 2023-05-17 -- TIL: Hashing data in the form of a UUID 2022-11-11 -- Using Alembic with SQLAlchemy 2 2022-08-22 -- Porting a Python web app to Go 2022-08-09 -- 'From scratch to online in production' in a single day, with Django - Part 3 2022-07-28 -- Making SQLite much faster in a local dev environment 2022-07-26 -- Triggering a GitHub Action from an external source 2022-07-22 -- 'From scratch to online in production' in a single day, with Django - Part 2 2022-07-19 -- 'From scratch to online in production' in a single day, with Django - Part 1 2022-07-12 -- Building a Markdown-based blog 2022-07-10 -- Choosing a tech stack for my card game platform","title":"Olivier Philippon's devblog"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: TIL \u00b6 Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment blog \u00b6 Building a Markdown-based blog deployment \u00b6 'From scratch to online in production' in a single day, with Django - Part 3 django \u00b6 Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment 'From scratch to online in production' in a single day, with Django - Part 3 business_logic.py github \u00b6 Triggering a GitHub Action from an external source golang \u00b6 Choosing a tech stack for my card game platform Porting a Python web app to Go laravel \u00b6 Choosing a tech stack for my card game platform mkdocs \u00b6 Building a Markdown-based blog next.js \u00b6 Choosing a tech stack for my card game platform orm \u00b6 Using Alembic with SQLAlchemy 2 project layout \u00b6 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Porting a Python web app to Go python \u00b6 Building a Markdown-based blog Using Alembic with SQLAlchemy 2 TIL: Hashing data in the form of a UUID business_logic.py rails \u00b6 Choosing a tech stack for my card game platform sqlalchemy \u00b6 Using Alembic with SQLAlchemy 2 sqlite \u00b6 Making SQLite much faster in a local dev environment terminal \u00b6 Let's try Kitty layouts til \u00b6 TIL: Hashing data in the form of a UUID","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#til","text":"Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment","title":"TIL"},{"location":"tags/#blog","text":"Building a Markdown-based blog","title":"blog"},{"location":"tags/#deployment","text":"'From scratch to online in production' in a single day, with Django - Part 3","title":"deployment"},{"location":"tags/#django","text":"Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment 'From scratch to online in production' in a single day, with Django - Part 3 business_logic.py","title":"django"},{"location":"tags/#github","text":"Triggering a GitHub Action from an external source","title":"github"},{"location":"tags/#golang","text":"Choosing a tech stack for my card game platform Porting a Python web app to Go","title":"golang"},{"location":"tags/#laravel","text":"Choosing a tech stack for my card game platform","title":"laravel"},{"location":"tags/#mkdocs","text":"Building a Markdown-based blog","title":"mkdocs"},{"location":"tags/#nextjs","text":"Choosing a tech stack for my card game platform","title":"next.js"},{"location":"tags/#orm","text":"Using Alembic with SQLAlchemy 2","title":"orm"},{"location":"tags/#project-layout","text":"'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Porting a Python web app to Go","title":"project layout"},{"location":"tags/#python","text":"Building a Markdown-based blog Using Alembic with SQLAlchemy 2 TIL: Hashing data in the form of a UUID business_logic.py","title":"python"},{"location":"tags/#rails","text":"Choosing a tech stack for my card game platform","title":"rails"},{"location":"tags/#sqlalchemy","text":"Using Alembic with SQLAlchemy 2","title":"sqlalchemy"},{"location":"tags/#sqlite","text":"Making SQLite much faster in a local dev environment","title":"sqlite"},{"location":"tags/#terminal","text":"Let's try Kitty layouts","title":"terminal"},{"location":"tags/#til_1","text":"TIL: Hashing data in the form of a UUID","title":"til"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/","tags":["next.js","django","laravel","rails","golang"],"text":"Choosing a tech stack for my card game platform \u00b6 Here are some of the technologies I have professional experience with, in no particular order. Let's check their pros and cons for this card game platform project I want to start building... Note that these bullets points are strictly subjective and personal - I think that all these stacks are perfectly good and valid, and choosing one or another is always a good option anyhow! Next.js \u00b6 Pros Modern, evolving at a quick pace, tons of best practices built-in. Very well documented. I love TypeScript, I like React Now that it's rather mainstream there is a huge ecosystem for it One language (TypeScript) to rule them all! With new runtime such as Cloudflare Workers, Deno and Bun coming in, and all converging towards the use of standard Web APIs, it's a quite exciting time for JavaScript on the backend! Cons Even though it can be used a fullstack framework (as Theo, from ping.gg fame, explains here ) , and despite the numerous benefits of Node.js... I can't help thinking that Node.js doesn't give me the same level of productivity than what I can have by using a \"fully featured\" mature backend framework such as Rails, Laravel or Django - where all the core features such as a typical backend, like an ORM, database migrations, logging, etc, are features that are already plugged together out of the box I gave a quick shot at Prisma , the trendy Node.js ORM; even though it has some undeniable qualities I didn't really fall in love with it - for various reasons. Laravel \u00b6 Pros Probably the framework that comes with the highest number of features built-in. Asynchronous jobs, websockets, authentication, support of Vite.js, you name it... Laravel has it by default. Easy deployment: whether it's via Forge or Vapor, Laravel comes with charged-but-very-handy solutions for deployment. Cons PHP has been my main programming language for backend stuff for more than 15 years, and even though it's never been better than today I grew a bit tired of its $peculiarities Ruby On Rails \u00b6 Pros Almost as \"all features built-in\" as Laravel Huge and mature ecosystem Ruby is a very expressive language Cons After having worked with languages that have inlined type annotations during the last few years, I struggle with languages like Ruby which don't have this So much magic that it can be really hard sometimes to trace what method comes from where and really understand what's going on Django \u00b6 Pros Python My favourite ORM, with (for me) the right balance between power and pragmatism. Also comes with great support for modern database features, such as JSON operators. It's been my tech stack for the last 4 years, so I'm pretty productive with it Has excellent GraphQL libraries (and I love GraphQL! ^_^) The Django Admin website is a huge gain of time while prototyping stuff Cons Some old-school aspects (no routing via HTTP verbs for example - which can be solved by using Django REST Framework, but for some reason I've never really liked it ) Features such as websockets and asynchronous jobs are not built-in, but Django's ecosystem is vast enough to be able to add them with a good integrations ( Channels for the former and Dramatiq for the latter) Go \u00b6 Pros I love the minimalism of the language Very stable over time Strongly typed, with a very smart compiler It's really nice to work with a programming language that have features such as code formatting or testing built-in Cons Whether it's at the database or the GraphQL layer, null values have to be handled and I'm not a big fans of the solutions Go has to offer for that ( sql.NullString and friends, or pointers to primitive values) Can be really verbose sometimes","title":"Choosing a tech stack for my card game platform"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#choosing-a-tech-stack-for-my-card-game-platform","text":"Here are some of the technologies I have professional experience with, in no particular order. Let's check their pros and cons for this card game platform project I want to start building... Note that these bullets points are strictly subjective and personal - I think that all these stacks are perfectly good and valid, and choosing one or another is always a good option anyhow!","title":"Choosing a tech stack for my card game platform"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#nextjs","text":"Pros Modern, evolving at a quick pace, tons of best practices built-in. Very well documented. I love TypeScript, I like React Now that it's rather mainstream there is a huge ecosystem for it One language (TypeScript) to rule them all! With new runtime such as Cloudflare Workers, Deno and Bun coming in, and all converging towards the use of standard Web APIs, it's a quite exciting time for JavaScript on the backend! Cons Even though it can be used a fullstack framework (as Theo, from ping.gg fame, explains here ) , and despite the numerous benefits of Node.js... I can't help thinking that Node.js doesn't give me the same level of productivity than what I can have by using a \"fully featured\" mature backend framework such as Rails, Laravel or Django - where all the core features such as a typical backend, like an ORM, database migrations, logging, etc, are features that are already plugged together out of the box I gave a quick shot at Prisma , the trendy Node.js ORM; even though it has some undeniable qualities I didn't really fall in love with it - for various reasons.","title":"Next.js"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#laravel","text":"Pros Probably the framework that comes with the highest number of features built-in. Asynchronous jobs, websockets, authentication, support of Vite.js, you name it... Laravel has it by default. Easy deployment: whether it's via Forge or Vapor, Laravel comes with charged-but-very-handy solutions for deployment. Cons PHP has been my main programming language for backend stuff for more than 15 years, and even though it's never been better than today I grew a bit tired of its $peculiarities","title":"Laravel"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#ruby-on-rails","text":"Pros Almost as \"all features built-in\" as Laravel Huge and mature ecosystem Ruby is a very expressive language Cons After having worked with languages that have inlined type annotations during the last few years, I struggle with languages like Ruby which don't have this So much magic that it can be really hard sometimes to trace what method comes from where and really understand what's going on","title":"Ruby On Rails"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#django","text":"Pros Python My favourite ORM, with (for me) the right balance between power and pragmatism. Also comes with great support for modern database features, such as JSON operators. It's been my tech stack for the last 4 years, so I'm pretty productive with it Has excellent GraphQL libraries (and I love GraphQL! ^_^) The Django Admin website is a huge gain of time while prototyping stuff Cons Some old-school aspects (no routing via HTTP verbs for example - which can be solved by using Django REST Framework, but for some reason I've never really liked it ) Features such as websockets and asynchronous jobs are not built-in, but Django's ecosystem is vast enough to be able to add them with a good integrations ( Channels for the former and Dramatiq for the latter)","title":"Django"},{"location":"2022/07-10---choosing-a-tech-stack-for-my-card-game-platform/#go","text":"Pros I love the minimalism of the language Very stable over time Strongly typed, with a very smart compiler It's really nice to work with a programming language that have features such as code formatting or testing built-in Cons Whether it's at the database or the GraphQL layer, null values have to be handled and I'm not a big fans of the solutions Go has to offer for that ( sql.NullString and friends, or pointers to primitive values) Can be really verbose sometimes","title":"Go"},{"location":"2022/07-12---building-a-markdown-based-blog/","tags":["blog","mkdocs","python"],"text":"I've put this devblog online the other day, but haven't documented how I built it yet - which will be useful for my future self if I have to do something like this again, and could potentially be of some help for other people. Here is how I determined what was the quickest way (for me) to put a \"devblog\" online as quickly as possible, without hours of setup. Why not Next.js? \u00b6 My natural \"go-to\" option would be Next.js, with its SSG capacities and built-in support of MDX . Also, I like managing this kind of stuff manually myself - i.e. traversing a folder of Markdown files at deployment time when Vercel runs npm run build , sorting my content, providing the result to getStaticProps ... It's genuinely fun However, even though I really enjoy doing this kind of work on the Node.js side there is a drawback for me: I would then have to build the user interface myself. And my skills in terms of design being as bad as they can be , I really don't want to work for hours on the React side only to end up with an ugly UI. There are open source or free turnkey blog templates for React/Next.js, of course, but I haven't really seen any of their design that I like (that's obviously highly subjective). Hence my second option... Material for MKDocs \u00b6 Or: Using a documentation engine to make a blog, what could go wrong? (spoiler: it was quick and fun to do, I regret nothing :-) MkDocs and its modern Material theme \u00b6 MkDocs is a static documentation generator, programmed in Python and configurable in YAML, created (if I'm not wrong) by Tom Christie - who's also behind huge Python projects like Django REST Framework or Starlette . Question I don't know the history of MkDocs, but I guess Tom Christie created it to document Django REST Framework? Material for MKDocs is a theme for MKDocs, based on Google's Material UI guidelines, that make MKDocs look much more modern (as great as the project is, MKDocs' default theme shows its age nowadays). Why they can be relevant to build a simple devblog \u00b6 Neither MKDocs or Material for MKDocs are designed to make blogs (even though it's on the roadmap ), but they have some common aspects with what I wanted to do: Treats folders of Markdown files (with YAML metadata in their header) as a tree of Web pages Built-in ability to publish static pages to GitHub Pages , for free Simple but good-looking user interface theme by default , so I don't have to fiddle with HTML and CSS but can focus on the content itself Made with a technology I'm familiar with (Python in this case) Excellent syntax highlighting (will be useful for my snippets :-), powered by Pygments The plugin mkdocs-awesome-pages-plugin also helps, so I don't have to build the whole navigation tree manually in the mkdocs.yml file Quick setup of \" Material for MKDocs as a blog\" \u00b6 I found two good resources explaining how to use Material for MKDocs as a blog engine, so I could hit the ground running (I really didn't want to spend hours on this setup ): https://www.dirigible.io/blogs/2021/11/2/material-blogging-capabilities/ https://ultrabug.fr/Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/ The documentation of Material for MKDocs itself if really nice, and pragmatic - explaining very simply for example how one can publish the generated static HTML content to GitHub Pages: https://squidfunk.github.io/mkdocs-material/publishing-your-site It was looking doable! :-) Let's give it a shot, with a quick Python setup: $ mkdir devblog && cd devblog/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) mkdocs-material \\ mkdocs-awesome-pages-plugin let's use a recent version of Python create a virtual env in a \".venv\" folder activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder in the virtual env, update pip and install the Poetry package manager initialise Poetry for this project ask Poetry to install the few packages we need for this blog Now all I had to do was to follow what these 2 articles were explaining, browse a bit the Material for MKDocs documentation, use the really nice icons and emojis search provided by this documentation (it's the little things ^_^)... and a couple of hours later my first blog post was online, automatically published by a GitHub Action every time I push my main branch! My quick personal touch \u00b6 Compared to these two very useful articles, my only personal touch was to add two things I always use in my projects: The modern package manager Poetry rather than pip (the Python default one) Create a Makefile at the root of the git repository, for the common tasks I also wanted to automate the \"posts table of content\" on the blog homepage. I chose to do it myself, mainly because it's the kind of things I really enjoy coding ^_^ The logic lives in the my_plugins/blog_toc/hooks.py Python file, plugged to the MKDocs generation lifecycle with the nice mkdocs-simple-hooks package for the sake of simplicity Later on I added the generation of a RSS feed when the blog is built, powered by the mkdocs-rss-plugin package.","title":"Building a Markdown-based blog"},{"location":"2022/07-12---building-a-markdown-based-blog/#why-not-nextjs","text":"My natural \"go-to\" option would be Next.js, with its SSG capacities and built-in support of MDX . Also, I like managing this kind of stuff manually myself - i.e. traversing a folder of Markdown files at deployment time when Vercel runs npm run build , sorting my content, providing the result to getStaticProps ... It's genuinely fun However, even though I really enjoy doing this kind of work on the Node.js side there is a drawback for me: I would then have to build the user interface myself. And my skills in terms of design being as bad as they can be , I really don't want to work for hours on the React side only to end up with an ugly UI. There are open source or free turnkey blog templates for React/Next.js, of course, but I haven't really seen any of their design that I like (that's obviously highly subjective). Hence my second option...","title":"Why not Next.js?"},{"location":"2022/07-12---building-a-markdown-based-blog/#material-for-mkdocs","text":"Or: Using a documentation engine to make a blog, what could go wrong? (spoiler: it was quick and fun to do, I regret nothing :-)","title":"Material for MKDocs"},{"location":"2022/07-12---building-a-markdown-based-blog/#mkdocs-and-its-modern-material-theme","text":"MkDocs is a static documentation generator, programmed in Python and configurable in YAML, created (if I'm not wrong) by Tom Christie - who's also behind huge Python projects like Django REST Framework or Starlette . Question I don't know the history of MkDocs, but I guess Tom Christie created it to document Django REST Framework? Material for MKDocs is a theme for MKDocs, based on Google's Material UI guidelines, that make MKDocs look much more modern (as great as the project is, MKDocs' default theme shows its age nowadays).","title":"MkDocs and its modern Material theme"},{"location":"2022/07-12---building-a-markdown-based-blog/#why-they-can-be-relevant-to-build-a-simple-devblog","text":"Neither MKDocs or Material for MKDocs are designed to make blogs (even though it's on the roadmap ), but they have some common aspects with what I wanted to do: Treats folders of Markdown files (with YAML metadata in their header) as a tree of Web pages Built-in ability to publish static pages to GitHub Pages , for free Simple but good-looking user interface theme by default , so I don't have to fiddle with HTML and CSS but can focus on the content itself Made with a technology I'm familiar with (Python in this case) Excellent syntax highlighting (will be useful for my snippets :-), powered by Pygments The plugin mkdocs-awesome-pages-plugin also helps, so I don't have to build the whole navigation tree manually in the mkdocs.yml file","title":"Why they can be relevant to build a simple devblog"},{"location":"2022/07-12---building-a-markdown-based-blog/#quick-setup-of-material-for-mkdocs-as-a-blog","text":"I found two good resources explaining how to use Material for MKDocs as a blog engine, so I could hit the ground running (I really didn't want to spend hours on this setup ): https://www.dirigible.io/blogs/2021/11/2/material-blogging-capabilities/ https://ultrabug.fr/Tech%20Blog/2021/2021-07-28-create-beautiful-and-localized-documentations-and-websites-using-mkdocs-github/ The documentation of Material for MKDocs itself if really nice, and pragmatic - explaining very simply for example how one can publish the generated static HTML content to GitHub Pages: https://squidfunk.github.io/mkdocs-material/publishing-your-site It was looking doable! :-) Let's give it a shot, with a quick Python setup: $ mkdir devblog && cd devblog/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) mkdocs-material \\ mkdocs-awesome-pages-plugin let's use a recent version of Python create a virtual env in a \".venv\" folder activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder in the virtual env, update pip and install the Poetry package manager initialise Poetry for this project ask Poetry to install the few packages we need for this blog Now all I had to do was to follow what these 2 articles were explaining, browse a bit the Material for MKDocs documentation, use the really nice icons and emojis search provided by this documentation (it's the little things ^_^)... and a couple of hours later my first blog post was online, automatically published by a GitHub Action every time I push my main branch!","title":"Quick setup of \"Material for MKDocs as a blog\""},{"location":"2022/07-12---building-a-markdown-based-blog/#my-quick-personal-touch","text":"Compared to these two very useful articles, my only personal touch was to add two things I always use in my projects: The modern package manager Poetry rather than pip (the Python default one) Create a Makefile at the root of the git repository, for the common tasks I also wanted to automate the \"posts table of content\" on the blog homepage. I chose to do it myself, mainly because it's the kind of things I really enjoy coding ^_^ The logic lives in the my_plugins/blog_toc/hooks.py Python file, plugged to the MKDocs generation lifecycle with the nice mkdocs-simple-hooks package for the sake of simplicity Later on I added the generation of a RSS feed when the blog is built, powered by the mkdocs-rss-plugin package.","title":"My quick personal touch"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/","tags":["django","project layout"],"text":"'From scratch to online in production' in a single day, with Django - Part 1 \u00b6 A Gin Rummy leaderboard \u00b6 There is this card game I play a lot with my partner, when we go to the pub or just chill outside: Gin Rummy . If you're curious about the game itself... Gin Rummy is a two-player card game, created by a professional card game player and his son in 1909 - preferably to be played in pubs, as its name made of 2 alcohols' ones hints It's easy to learn, the rounds are pretty quick, and even though randomness always plays an important role in card games its role is not too strong in Gin Rummy. A proof of this is that during the 1970s and 1980s there used to be a champion, Stu Ungar , who had such a total dominance of the game that it actually ended up killing the game in tournaments, since he was pretty much winning all of them! We like playing casually, but we also like trying to do our best while playing, so we had to find a handy way to keep track of our scores to keep a bit of competitiveness. As a Web developer, of course I had to build a leaderboard for us! My challenge: start working on it in the morning, and have it live in production in the afternoon \u00b6 My challenge was to build it in one single day , from scratch in the morning to having it online up and running with a database in the afternoon. The stack I'm the most productive with being Django , I opted for this framework despite the minimalism of the project. Is Django a good choice for small projects? Yes! Contrary to frameworks such a Ruby On Rails or Laravel, which tends to create a lot fo files for a blank project, creating a new Django project with django-admin startproject creates only 6 files , making it a good choice even for small projects. One can even create a Django-powered REST API contained in a single Python file! https://adamj.eu/tech/2020/10/15/a-single-file-rest-api-in-django/ This is the first post explaining how I typically organise the file tree of a Django project - this layout does the job for a micro project such as this Gin Rummy leaderboard, but scales very well for \"real life\" projects too! I have several Django apps running in production for years with this layout, and the pattern scales smoothly as features keep being added to the projects Let's start! My typical bootstrap of a Django app \u00b6 $ mkdir gin-scoring && cd gin-scoring/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) Django \\ django-environ \\ # (7) psycopg2 \\ # (8) Jinja2 # (9) Let's use a recent version of Python Create a virtual env in a \".venv\" folder Activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder In the virtual env, update pip and install the Poetry package manager Initialise Poetry for this project Ask Poetry to install the few packages we need for this blog: Install django-environ to manage our settings I'll be using Postgresql for the database, so we need a Python driver for it I was never a big fan of the Django built-in templating language, so I always use Jinja instead Installing pip and Poetry in the virtual environment These 2 tools can be used globally , and do not require such a local installation. The reason I do this is just because I like having all my Python projects entirely self-contained in their respective virtual envs. So if one of my project uses Poetry 2.x one day, for example, I'll be able to use it on new projects without messing up my existing ones Once we have this we can bootstrap the project: ( .venv ) $ mkdir src && cd src/ # (1) ( .venv ) $ django-admin startproject \"project\" . I always put my Python files in a \"src/\" folder With this startproject command we create the skeleton of a Django project in the current folder ( src/ ), with a project named \"project\" - I always choose this name because of the way I handle my Django settings. I like having a apps/ and a project/ folders in my src/ one: the former will be the Python package where all my Django app code lives, while the latter is where I'll store the project-wide settings. In a nutshell, the file tree I want to have is this: gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ # (1) \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 models.py \u2502 \u2502 \u2514\u2500\u2500 gin_scoring/ # (2) \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 urls.py \u2502 \u251c\u2500\u2500 project/ # (3) \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 _base.py # (4) \u2502 \u2502 \u2502 \u251c\u2500\u2500 development.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 flyio.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 heroku.py \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (5) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py # (6) \u2502 \u2514\u2500\u2500 manage.py* # (7) \u251c\u2500\u2500 tests/ \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml This will be a apps.authentication package This will be a apps.gin_scoring package - where the business logic of this mini project will be :-) This is why I use \"project\" for the name of my Django project when I boostrap it with django-admin startproject The \"settings\" file generated by django-admin startproject : I just move it in this folder and rename it into _base.py The ASGI (async Python) entry point of the project, generated by Django - I'm not using it at the moment, but it could be useful later on so let's keep it :-) The WSGI (\"traditional\" Python) entry point: that's where all the HTTP requests of this app will be processed The classic command line entry point for Django - from now on I will always use python src/manage.py for my Django commands, and startproject was the only one for which I used django-admin (file tree generated with tree --dirsfirst -I __pycache__ -F -L 4 . - see tree 's MAN page) Having all my Django apps namespaced in this apps package allows me to use pretty much any name for them, without any risks of collisions with a 3rd-party package. Why I namespace my Python code In environments such as PHP or Node.js the 3rd-party packages we add to a project are always namespaced, so there are no risks of collisions with our own code. On a Laravel project for example, the HTTP request class will have the fully-qualified name Illuminate\\Http\\Request ; so we're free to have our own Request class with pretty much any prefix we want, if we need one in order to follow the \"domain\" glossary of the project we're building. However, in the Python world there is not only no namespacing, but also no constrained matching between the name of a 3rd-party package we add to a project and its Python package . For example, the package django-environ lives in a package named environ . This is why I tend to be a bit defensive when it comes to namespace my own code In that aspect I actually imitate what a typical Laravel project does, since everything is namespaced in a top-level \\App\\ namespace there: https://laravel.com/docs/9.x/structure#the-app-directory I can still have collisions in the Django apps names though - but it's a less annoying risk. Example of a Django apps names collision Django comes with a handy auth app that I want to use, which I why I cannot create an app that have this name myself - hence my longer apps.authentication naming Django settings management \u00b6 The core of my Django settings are in the file src/project/settings/_base.py : this is just the settings file generated by django-admin startproject , that I moved into a new settings/ folder and renamed - the leading underscore is just common a convention to emphasise that this Python module shouldn't itself be imported. It all starts with a _base \u00b6 The content of this file looks like this: import os from pathlib import Path import environ # This points to our git repo's root: BASE_DIR = Path ( __file__ ) . parent . resolve () / \"..\" / \"..\" / \"..\" env = environ . Env () if os . environ . get ( \"USE_DOT_ENV\" ): # (1) for env_file_name in ( \".env\" , \".env.local\" ): env_file_path = BASE_DIR / env_file_name try : environ . Env . read_env ( env_file_path ) except ( OSError , AttributeError ): pass # no .env file? No problem! SECRET_KEY = env . str ( \"SECRET_KEY\" ) # Classic Django settings, generated by `django-admin startproject` # I'll omit them for brevity INSTALLED_APPS = [ ... ] MIDDLEWARE = [ ... ] ROOT_URLCONF = \"project.urls\" # etc. # Database # https://docs.djangoproject.com/en/4.0/ref/settings/#databases DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), # (2) } # etc., again... I will only use this in \"local\" development: in production the settings are only set via environment variables And here where django-environ kicks in again: we use a single DATABASE_URL environment var, rather than one setting for the username, one for the password, etc. From there, we just have to define \"environment-specific\" settings. \"Local dev\" settings \u00b6 My local development settings look like this for example: import os # This enables the loading of \".env\" files in local development: os . environ [ \"USE_DOT_ENV\" ] = \"YES\" # N.B. This is the only part of my Python code # where I allow myself \"star imports\" :-) from ._base import * DEBUG = True ALLOWED_HOSTS = [] LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"DJANGO_LOG_LEVEL\" , default = \"WARNING\" ), }, \"loggers\" : { \"apps\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"APP_LOG_LEVEL\" , default = \"INFO\" ), \"propagate\" : False , }, \"django.db.backends\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"SQL_LOG_LEVEL\" , default = \"WARNING\" ), # (1) \"propagate\" : False , }, }, } Such granular logging is very handy in development mode :-) So if I want to check the SQL queries generated by the Django ORM while I'm working on the project, all I have to do is to launch my server with: ( .venv ) $ SQL_LOG_LEVEL = DEBUG djm runserver The djm shell alias djm is short for \"DJango Management\" - it's an alias I have in my shell's startup file. As I always use this same layout for all my Django projects I can always use the same alias to run my Django commands. alias djm = 'DJANGO_SETTINGS_MODULE=project.settings.development python src/manage.py' So from there I can start my Django server with djm runserver , generate database migrations with djm makemigrations , apply them with djm migrate , etc. The venv shell alias Oh, and while we're there, here is another handy alias: alias venv = 'source .venv/bin/activate' So when I cd into a Python project folder I just have to type venv to activate its virtual environment, as I always create it in a .venv/ folder Production settings \u00b6 My production (Heroku in this case) settings, in the same folder, look like that: from ._base import * ALLOWED_HOSTS = env . list ( \"ALLOWED_HOSTS\" ) DEBUG = False SECURE_SSL_REDIRECT = True CSRF_COOKIE_SECURE = True SESSION_COOKIE_SECURE = True # Static assets served by Whitenoise on production # @link https://devcenter.heroku.com/articles/django-assets # @link http://whitenoise.evans.io/en/stable/ STATIC_ROOT = BASE_DIR / \"staticfiles\" MIDDLEWARE . append ( \"whitenoise.middleware.WhiteNoiseMiddleware\" ) STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\" # Logging LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : \"WARNING\" , }, } In the next post we'll start coding the app itself, using this pattern I was mentioning - which shines by its simplicity and ability to scale as a project gets more and more complex. Thanks to my friend Yann - einenlum.com - for his review on this post Part 2 \u00b6 UPDATE: Part 2 is online","title":"'From scratch to online in production' in a single day, with Django - Part 1"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#from-scratch-to-online-in-production-in-a-single-day-with-django-part-1","text":"","title":"'From scratch to online in production' in a single day, with Django - Part 1"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#a-gin-rummy-leaderboard","text":"There is this card game I play a lot with my partner, when we go to the pub or just chill outside: Gin Rummy . If you're curious about the game itself... Gin Rummy is a two-player card game, created by a professional card game player and his son in 1909 - preferably to be played in pubs, as its name made of 2 alcohols' ones hints It's easy to learn, the rounds are pretty quick, and even though randomness always plays an important role in card games its role is not too strong in Gin Rummy. A proof of this is that during the 1970s and 1980s there used to be a champion, Stu Ungar , who had such a total dominance of the game that it actually ended up killing the game in tournaments, since he was pretty much winning all of them! We like playing casually, but we also like trying to do our best while playing, so we had to find a handy way to keep track of our scores to keep a bit of competitiveness. As a Web developer, of course I had to build a leaderboard for us!","title":"A Gin Rummy leaderboard"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#my-challenge-start-working-on-it-in-the-morning-and-have-it-live-in-production-in-the-afternoon","text":"My challenge was to build it in one single day , from scratch in the morning to having it online up and running with a database in the afternoon. The stack I'm the most productive with being Django , I opted for this framework despite the minimalism of the project. Is Django a good choice for small projects? Yes! Contrary to frameworks such a Ruby On Rails or Laravel, which tends to create a lot fo files for a blank project, creating a new Django project with django-admin startproject creates only 6 files , making it a good choice even for small projects. One can even create a Django-powered REST API contained in a single Python file! https://adamj.eu/tech/2020/10/15/a-single-file-rest-api-in-django/ This is the first post explaining how I typically organise the file tree of a Django project - this layout does the job for a micro project such as this Gin Rummy leaderboard, but scales very well for \"real life\" projects too! I have several Django apps running in production for years with this layout, and the pattern scales smoothly as features keep being added to the projects Let's start!","title":"My challenge: start working on it in the morning, and have it live in production in the afternoon"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#my-typical-bootstrap-of-a-django-app","text":"$ mkdir gin-scoring && cd gin-scoring/ $ pyenv shell 3 .10.4 # (1) $ python -m venv .venv # (2) $ source .venv/bin/activate # (3) ( .venv ) $ pip install -U pip poetry # (4) ( .venv ) $ poetry init # (5) ( .venv ) $ poetry add \\ # (6) Django \\ django-environ \\ # (7) psycopg2 \\ # (8) Jinja2 # (9) Let's use a recent version of Python Create a virtual env in a \".venv\" folder Activate the virtual env: from now on the Python-related commands we type only impact the \".venv\" folder In the virtual env, update pip and install the Poetry package manager Initialise Poetry for this project Ask Poetry to install the few packages we need for this blog: Install django-environ to manage our settings I'll be using Postgresql for the database, so we need a Python driver for it I was never a big fan of the Django built-in templating language, so I always use Jinja instead Installing pip and Poetry in the virtual environment These 2 tools can be used globally , and do not require such a local installation. The reason I do this is just because I like having all my Python projects entirely self-contained in their respective virtual envs. So if one of my project uses Poetry 2.x one day, for example, I'll be able to use it on new projects without messing up my existing ones Once we have this we can bootstrap the project: ( .venv ) $ mkdir src && cd src/ # (1) ( .venv ) $ django-admin startproject \"project\" . I always put my Python files in a \"src/\" folder With this startproject command we create the skeleton of a Django project in the current folder ( src/ ), with a project named \"project\" - I always choose this name because of the way I handle my Django settings. I like having a apps/ and a project/ folders in my src/ one: the former will be the Python package where all my Django app code lives, while the latter is where I'll store the project-wide settings. In a nutshell, the file tree I want to have is this: gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ # (1) \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 models.py \u2502 \u2502 \u2514\u2500\u2500 gin_scoring/ # (2) \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 urls.py \u2502 \u251c\u2500\u2500 project/ # (3) \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 _base.py # (4) \u2502 \u2502 \u2502 \u251c\u2500\u2500 development.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 flyio.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 heroku.py \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (5) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py # (6) \u2502 \u2514\u2500\u2500 manage.py* # (7) \u251c\u2500\u2500 tests/ \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml This will be a apps.authentication package This will be a apps.gin_scoring package - where the business logic of this mini project will be :-) This is why I use \"project\" for the name of my Django project when I boostrap it with django-admin startproject The \"settings\" file generated by django-admin startproject : I just move it in this folder and rename it into _base.py The ASGI (async Python) entry point of the project, generated by Django - I'm not using it at the moment, but it could be useful later on so let's keep it :-) The WSGI (\"traditional\" Python) entry point: that's where all the HTTP requests of this app will be processed The classic command line entry point for Django - from now on I will always use python src/manage.py for my Django commands, and startproject was the only one for which I used django-admin (file tree generated with tree --dirsfirst -I __pycache__ -F -L 4 . - see tree 's MAN page) Having all my Django apps namespaced in this apps package allows me to use pretty much any name for them, without any risks of collisions with a 3rd-party package. Why I namespace my Python code In environments such as PHP or Node.js the 3rd-party packages we add to a project are always namespaced, so there are no risks of collisions with our own code. On a Laravel project for example, the HTTP request class will have the fully-qualified name Illuminate\\Http\\Request ; so we're free to have our own Request class with pretty much any prefix we want, if we need one in order to follow the \"domain\" glossary of the project we're building. However, in the Python world there is not only no namespacing, but also no constrained matching between the name of a 3rd-party package we add to a project and its Python package . For example, the package django-environ lives in a package named environ . This is why I tend to be a bit defensive when it comes to namespace my own code In that aspect I actually imitate what a typical Laravel project does, since everything is namespaced in a top-level \\App\\ namespace there: https://laravel.com/docs/9.x/structure#the-app-directory I can still have collisions in the Django apps names though - but it's a less annoying risk. Example of a Django apps names collision Django comes with a handy auth app that I want to use, which I why I cannot create an app that have this name myself - hence my longer apps.authentication naming","title":"My typical bootstrap of a Django app"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#django-settings-management","text":"The core of my Django settings are in the file src/project/settings/_base.py : this is just the settings file generated by django-admin startproject , that I moved into a new settings/ folder and renamed - the leading underscore is just common a convention to emphasise that this Python module shouldn't itself be imported.","title":"Django settings management"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#it-all-starts-with-a-_base","text":"The content of this file looks like this: import os from pathlib import Path import environ # This points to our git repo's root: BASE_DIR = Path ( __file__ ) . parent . resolve () / \"..\" / \"..\" / \"..\" env = environ . Env () if os . environ . get ( \"USE_DOT_ENV\" ): # (1) for env_file_name in ( \".env\" , \".env.local\" ): env_file_path = BASE_DIR / env_file_name try : environ . Env . read_env ( env_file_path ) except ( OSError , AttributeError ): pass # no .env file? No problem! SECRET_KEY = env . str ( \"SECRET_KEY\" ) # Classic Django settings, generated by `django-admin startproject` # I'll omit them for brevity INSTALLED_APPS = [ ... ] MIDDLEWARE = [ ... ] ROOT_URLCONF = \"project.urls\" # etc. # Database # https://docs.djangoproject.com/en/4.0/ref/settings/#databases DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), # (2) } # etc., again... I will only use this in \"local\" development: in production the settings are only set via environment variables And here where django-environ kicks in again: we use a single DATABASE_URL environment var, rather than one setting for the username, one for the password, etc. From there, we just have to define \"environment-specific\" settings.","title":"It all starts with a _base"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#local-dev-settings","text":"My local development settings look like this for example: import os # This enables the loading of \".env\" files in local development: os . environ [ \"USE_DOT_ENV\" ] = \"YES\" # N.B. This is the only part of my Python code # where I allow myself \"star imports\" :-) from ._base import * DEBUG = True ALLOWED_HOSTS = [] LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"DJANGO_LOG_LEVEL\" , default = \"WARNING\" ), }, \"loggers\" : { \"apps\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"APP_LOG_LEVEL\" , default = \"INFO\" ), \"propagate\" : False , }, \"django.db.backends\" : { \"handlers\" : [ \"console\" ], \"level\" : env . str ( \"SQL_LOG_LEVEL\" , default = \"WARNING\" ), # (1) \"propagate\" : False , }, }, } Such granular logging is very handy in development mode :-) So if I want to check the SQL queries generated by the Django ORM while I'm working on the project, all I have to do is to launch my server with: ( .venv ) $ SQL_LOG_LEVEL = DEBUG djm runserver The djm shell alias djm is short for \"DJango Management\" - it's an alias I have in my shell's startup file. As I always use this same layout for all my Django projects I can always use the same alias to run my Django commands. alias djm = 'DJANGO_SETTINGS_MODULE=project.settings.development python src/manage.py' So from there I can start my Django server with djm runserver , generate database migrations with djm makemigrations , apply them with djm migrate , etc. The venv shell alias Oh, and while we're there, here is another handy alias: alias venv = 'source .venv/bin/activate' So when I cd into a Python project folder I just have to type venv to activate its virtual environment, as I always create it in a .venv/ folder","title":"\"Local dev\" settings"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#production-settings","text":"My production (Heroku in this case) settings, in the same folder, look like that: from ._base import * ALLOWED_HOSTS = env . list ( \"ALLOWED_HOSTS\" ) DEBUG = False SECURE_SSL_REDIRECT = True CSRF_COOKIE_SECURE = True SESSION_COOKIE_SECURE = True # Static assets served by Whitenoise on production # @link https://devcenter.heroku.com/articles/django-assets # @link http://whitenoise.evans.io/en/stable/ STATIC_ROOT = BASE_DIR / \"staticfiles\" MIDDLEWARE . append ( \"whitenoise.middleware.WhiteNoiseMiddleware\" ) STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\" # Logging LOGGING = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , }, }, \"root\" : { \"handlers\" : [ \"console\" ], \"level\" : \"WARNING\" , }, } In the next post we'll start coding the app itself, using this pattern I was mentioning - which shines by its simplicity and ability to scale as a project gets more and more complex. Thanks to my friend Yann - einenlum.com - for his review on this post","title":"Production settings"},{"location":"2022/07-19---from-scratch-to-online-in-production-in-a-single-day-with-django-part-1/#part-2","text":"UPDATE: Part 2 is online","title":"Part 2"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/","tags":["django","project layout"],"text":"'From scratch to online in production' in a single day, with Django - Part 2 \u00b6 Quick summary of the previous part \u00b6 In part 1 we saw a way to set up a Django project in a quite generic way, that can be used for any kind of project: Dependencies are managed by Poetry Settings that differ from an environment to another come from environment variables, and are populated from optional .env files in \"local development\" mode, powered by django-environ . SECRET_KEY = env . str ( \"SECRET_KEY\" ) DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), } On top of this we also have a Python module for each type of environment - so we'll use the DJANGO_SETTINGS_MODULE environment variable to point to project.settings.development during local development , project.settings.heroku on Heroku, project.settings.test when running our test suite, etc. Info Having such multiple settings files is a best practice I learned by reading the great book Two Scoops of Django - which itself cites this talk from Jacob Kaplan-Moss. A src/ folder contains all the application code, within 2 top-level packages for our Python modules: project , which contains the Django settings and the WSGI/ASGI HTTP entrypoints apps for the Django apps - so each of them is free to have any name we want, without any risks of collisions with an existing 3rd-party package. Next step: we have to organise the business logic inside our Django apps \u00b6 Django has some built-in recommendations about how to structure a project. Its applications concept for example is a good start to split the code into smaller units, and some basic rules like this one are good guidelines: Quote If there are 20+ models in a single app, think about ways to break it down into smaller apps, as it probably means your app is doing too much. In practice, we like to lower this number to no more than five to ten models per app . From Two Scoops of Django Now, let's take a look at an aspect that may be a mystery for other developers who enter this new ecosystem they're not familiar with yet - like me a few years ago, when I switched from \"PHP + Symfony \" to \"Python + Django \" : How to organise the business logic inside each Django app? Me, as I was learning Django Well, I guess \"the Django way\" would probably be to follow the Active Record pattern and write most of the code for this in the Models themselves - as well as in their Managers (which are similar to \"repositories\" or \"entity managers\" when using the Data Mapper pattern) However, in my own (subjective) case I generally find that doing so doesn't scale very well as the project grows, as we end up having all the business logic grouped in a bunch of huge classes. Which is why, as I was learning Django, I was quite eager to find another way to structure my code... An efficient pattern for the business logic: simplicity that scales well \u00b6 When I started to learn Django I was lucky enough to stumble upon this Django Styleguide , published on GitHub by the software development company HackSoft. And more specifically this part, where they explain \"services\" and \"selectors\": https://github.com/HackSoftware/Django-Styleguide#services My own adaptation of the \"services\" and \"selectors\" concept from HackSoft \u00b6 As I'm mostly working with GraphQL in my day-to-day job, I opted for a terminology that rings a bell a bit more to my ears than \"services\" and \"selectors\" : mutations and queries . The former is a package that contains code that alter a database (adding, modifying or deleting data from it), while the latter is specialised in fetching data. Here is how it looks like applied to my \"Gin Rummy leaderboard\" mini project: (the parts of the tree that don't matter in this case are replaced with three dots) gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 gin_scoring/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 mutations/ # (1) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _save_game_result.py \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 queries/ # (2) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame_monthly.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _last_game_results.py \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 gin_rummy.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 views.py \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 project/ \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (3) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u2514\u2500\u2500 manage.py* \u251c\u2500\u2500 tests/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml In this package we have the code that alters data In this package we have the code that fetches data asgi.py , urls.py and wsgi.py were created by the startproject command. jinja2.py is where I quickly configure Jinja, following the Django documentation: > https://docs.djangoproject.com/en/4.0/topics/templates/#django.template.backends.jinja2.Jinja2 We can see that for each Django app we have 2 sub-packages: domain.mutations is where the code that alter data lives domain.queries is where the code that fetch data lives They're both structured the same way, following these principles from HackSoft's styleguide: Each of their module exposes only one public function - and optionally some types if it has to, in order to describe the shape of its input and/or output. Each of these one-per-module-functions only accepts keyword arguments. These functions' signatures should be fully \"type hinted\". Each module is free to use as many private functions it needs to achieve the job described by its single public function. For the mutations, these functions' name should start with a verb , since they're the reflection of a business logic action Each module's name is prefixed with an underscore, to emphasise that it should not be imported directly The __init__.py file is in charge of exposing the public function of each module to the \"outside world\" - i.e. the Python code that doesn't live in the same package. Info The goal of the last 2 points is to avoid this kind of imports, where we have to repeat the same name twice - once of the module name and once for the function itself: from .domain.mutations.save_game_result import save_game_result So instead of this repetition we can simply do: from .domain.mutations import save_game_result A concrete example, with the Gin Rummy leaderboard app \u00b6 Ok, enough theory - let's see how that works in the context of this mini project! We have a single Django model , that looks like this: class GameResult ( models . Model ): player_north_name = models . CharField ( max_length = 50 ) player_south_name = models . CharField ( max_length = 50 ) outcome = models . CharField ( max_length = 10 , choices = [( outcome , outcome ) for outcome in GAME_OUTCOME . __args__ ]) # type: ignore # These 2 ones can be `null` when the outcome is `draw`: winner_name = models . CharField ( max_length = 50 , null = True ) deadwood_value = models . PositiveSmallIntegerField ( null = True ) # Computed from the previous `outcome` and `deadwood_value` fields: winner_score = models . PositiveSmallIntegerField ( null = True ) created_at = models . DateTimeField ( default = timezone . now ) @property def is_draw ( self ) -> bool : return self . outcome == \"draw\" @cached_property def loser_name ( self ) -> str | None : if self . is_draw : return None return [ name for name in ( self . player_north_name , self . player_south_name ) if name != self . winner_name ][ 0 ] def __str__ ( self ) -> str : return f \" { self . player_north_name . title () } vs { self . player_south_name . title () } , on { self . created_at . strftime ( ' %a %d %b at %H:%M' ) } \" The domain.mutations package of our Django app \u00b6 For this minimalist project we need only one mutation, which is triggered when the user submits the \"New game result\" HTML form: # file: src/apps/gin_scoring/domain/_save_game_result.py from ...domain.gin_rummy import GAME_OUTCOME , calculate_round_score from ...models import GameResult def save_game_result ( * , # (1) player_north_name : str , player_south_name : str , outcome : GAME_OUTCOME , # (2) winner_name : str | None , deadwood_value : int , ) -> GameResult : is_draw = outcome == \"draw\" winner_score = None if is_draw : winner_name = None else : winner_score = calculate_round_score ( game_outcome = outcome , deadwood_value = deadwood_value ) game_result_model = GameResult ( # (3) player_north_name = player_north_name , player_south_name = player_south_name , outcome = outcome , winner_name = winner_name , deadwood_value = deadwood_value , winner_score = winner_score , ) # (4) game_result_model . save () return game_result_model We force this function to be used only with the \"keyword arguments\" syntax GAME_OUTCOME is just a literal type, described later on in this same article To my knowledge there's no equivalent of the Shorthand property names of ES2015 in Python - which I guess must be on purpose, since readability is almost always the top priority of the language? For that reason we have to repeat the [field name]=[arg name] pattern, but in my opinion it's not really an issue The model also has a created_at field, but it will automatically be set by the Django ORM since we've used default=timezone.now when we defined the models.DateTimeField field And then, we expose that function to the rest of the Python code: # file: src/apps/gin_scoring/domain/mutations/__init__.py from ._save_game_result import save_game_result All we have to do now is to use that mutation from a Django view. There are several ways to do this, but here is an example: # file: src/apps/gin_scoring/views.py from .domain.mutations import save_game_result # ... @require_POST def post_game_result ( request : HttpRequest ) -> HttpResponse : try : game_result_payload = GameResultPayload ( ** request . POST . dict ()) except pydantic . ValidationError : return HttpResponseBadRequest () save_game_result ( # (1) player_north_name = game_result_payload . player_north_name , player_south_name = game_result_payload . player_south_name , outcome = game_result_payload . outcome , winner_name = game_result_payload . winner_name , deadwood_value = game_result_payload . deadwood_value , ) return redirect ( \"index\" ) Note that we could also have opted for a more concise way to transfer data from the \"validation and normalisation\" data structure to the mutation: save_game_result ( ** game_result_payload . dict () ) Validating the input of our Django views There are multiple ways to validate and normalise the input of our Django views, before passing its data to the \"domain\" layer. In this case I chose to use Pydantic . # file: src/apps/gin_scoring/http_payloads.py from typing import Any import pydantic from .domain.gin_rummy import GAME_OUTCOME from .helpers import normalize_player_name class GameResultPayload ( pydantic . BaseModel ): player_north_name : str player_south_name : str outcome : GAME_OUTCOME winner_name : str | None deadwood_value : int @pydantic . root_validator ( pre = True ) def normalize_player_names ( cls , values : dict [ str , Any ]): # In order to have consistent recording when players \"Rachel\" and \"Olivier\" add a game result, whether # \"Rachel\" is \"north\" and \"Olivier\" is \"south\" or vice-versa, we sort their names alphabetically # and then always set the \"north\" player to the first one and the \"south\" one to the second one: player_north_name , player_south_name = sorted ( ( normalize_player_name ( values [ \"player_north_name\" ]), normalize_player_name ( values [ \"player_south_name\" ]), ) ) values [ \"player_north_name\" ] = player_north_name values [ \"player_south_name\" ] = player_south_name return values @pydantic . validator ( \"winner_name\" ) def validate_winner_name ( cls , v : str , values : dict [ str , Any ]) -> str | None : is_draw = values [ \"outcome\" ] == \"draw\" if is_draw : return None # No winner name for \"draw\" games if not v : raise ValueError ( f \"non-draw games must have a winner name\" ) winner_name = normalize_player_name ( v ) player_names = ( values [ \"player_north_name\" ], values [ \"player_south_name\" ]) if winner_name not in player_names : raise ValueError ( f \"winner name { v } is not part of the players' names ' { ',' . join ( player_names ) } '\" ) return winner_name The domain.queries package of our Django app \u00b6 For this simple app we need only 3 queries: One to get the global \"Hall of fame\", where we determine the ranking of players based on all the games played so far One to get the monthly \"Hall of fame\", which does the same but with a distinct ranking for each month One that returns exhaustive data for the last 10 games that were played Let's take a look at the second one, for example: # file: src/apps/gin_scoring/domain/queries/_hall_of_fame_monthly.py from collections import defaultdict from datetime import datetime from typing import NamedTuple from django.db.models import Count , Sum from django.db.models.functions import TruncMonth from ...models import GameResult # (1) class HallOfFameMonthResult ( NamedTuple ): # (2) month : datetime winner_name : str game_counts : int win_counts : int score_delta : int def hall_of_fame_monthly () -> list [ HallOfFameMonthResult ]: # \u26a0\ufe0f Probably not the very best way to achieve this... # But this is a project I gave myself one single day to build, # so that will do the job \ud83d\ude05 # @link https://docs.djangoproject.com/en/4.0/topics/db/aggregation/ win_counts = Count ( \"winner_score\" ) total_score = Sum ( \"winner_score\" ) raw_results = ( GameResult . objects . filter ( winner_name__isnull = False ) . annotate ( month = TruncMonth ( \"created_at\" )) # (3) . values ( \"month\" , \"winner_name\" ) . distinct () . annotate ( win_counts = win_counts , total_score = total_score ) # Each won round is worth 25 points: . annotate ( grand_total = ( win_counts * 25 ) + total_score ) . order_by ( \"-month\" , \"-grand_total\" ) ) raw_results_per_month : dict [ datetime , list [ dict ]] = defaultdict ( list ) for raw_result in raw_results : raw_results_per_month [ raw_result [ \"month\" ]] . append ( raw_result ) returned_results : list [ HallOfFameMonthResult ] = [] for month , month_results in raw_results_per_month . items (): winner_result = month_results [ 0 ] winner_grand_total = winner_result [ \"grand_total\" ] or 0 second_best_grand_total = 0 if len ( month_results ) < 2 else ( month_results [ 1 ][ \"grand_total\" ] or 0 ) games_count = sum ([ res [ \"win_counts\" ] for res in month_results ]) returned_results . append ( HallOfFameMonthResult ( month = month , winner_name = winner_result [ \"winner_name\" ], game_counts = games_count , win_counts = winner_result [ \"win_counts\" ], score_delta = winner_grand_total - second_best_grand_total , ) ) return returned_results I like using relative imports for things we import that live in the same Django app . In all other cases I'd use absolute imports. We're not returning ActiveRecord items directly from a Django QuerySet there, but aggregated results. There are several data structures we can use in Python for that kind of \"value objects\", but I generally opt for typing.NamedTuple This is where we group the database rows by month Yes, I could probably have used itertools.groupby instead ^_^ Note Note that we could have split this into several functions - in which case they would all be private functions (their name would start with an underscore), and only the \"domain\" one would be public And similarly, the __init__.py file is in charge of exposing only what the rest of the Python # file: src/apps/gin_scoring/domain/queries/__init__.py from ._hall_of_fame import hall_of_fame from ._hall_of_fame_monthly import ( HallOfFameMonthResult , # (1) hall_of_fame_monthly ) from ._last_game_results import last_game_results # (2) Sometimes it's useful to not only export the one public function of the module, but also a dedicated type it's using for its input or output - so other Python modules can also use type hints when interacting with the \"domain\" layer But most of the time, all we need is to expose the public function of the module But what about the domain logic that is neither a mutation nor a query? \u00b6 We still have to put somewhere some parts of the domain don't fall in either categories. For example: Constants, enums, literal types... Data structures describing some aspects of the business logic, that can be used by both mutations and queries. Various forms of \"memory-only\" computations. Well... In my case, I find that in each Django app the domain package itself is a very good place to welcome these! For this mini project, for example, I chose to have a single Python module ( apps.gin_scoring.domain.gin_rummy ) to store some \"business-logic-related\" stuff that is specific to the Gin Rummy game , and that are neither a mutation nor a query: # file: src/apps/gin_scoring/domain/gin_rummy.py from typing import Literal # Possible outcomes of a Gin Rummy game: # (1) GAME_OUTCOME = Literal [ \"knock\" , \"gin\" , \"big_gin\" , \"undercut\" , \"draw\" ] def calculate_round_score ( * , game_outcome : GAME_OUTCOME , deadwood_value : int ) -> int : # @link https://en.wikipedia.org/wiki/Gin_rummy#Knocking match game_outcome : case \"draw\" : return 0 case \"knock\" : return deadwood_value case \"gin\" : return 25 + deadwood_value case \"big_gin\" : return 31 + deadwood_value case \"undercut\" : return 15 + deadwood_value case _ : raise ValueError ( f \"Invalid game outcome value ' { game_outcome } '\" ) More on that Literal type below We can see indeed that calculating the score of a round, depending on its outcome and the value of its deadwood , is neither a mutation nor a query: it's just a standalone computation, that does not depend on anything we would have in a database. And the same goes for enumerating the possible outcomes of a Gin Rummy game. Literals or Enums? To express the outcome of a Gin Rummy game I could of course have used a Python enum instead: @enum . unique class GameOutcome ( enum . Enum ): KNOCK = \"knock\" # or `enum.auto()` GIN = \"gin\" BIG_GIN = \"big_gin\" UNDERCUT = \"undercut\" DRAW = \"draw\" As for me, I must admit that I have no strict rules when I have to choose between one or another way to describe that kind of data I was mostly using Enums until a few years ago, but as I was using TypeScript more and more I realised that I really liked using literal types there - the TypeScript equivalent of that Python Literal would be: export type GAME_OUTCOME = \"knock\" | \"gin\" | \"big_gin\" | \"undercut\" | \"draw\" I appreciate the concision of literals, and tend to use them when I have the feeling that having such \"literal values\" spread in the code wouldn't cause any issue later on if I have to change their values Enums are certainly easier to handle in case of refactorings, but so far I've never come across a case where during a refactoring I regretted having opted for a literal rather than an Enum - fingers crossed, it won't be the case anytime soon! And that's it! \u00b6 As we can see the pattern is very simple to implement, and its few principles are a very good guideline for developers when they have to add some code. Is it code that creates, updates or deletes data in a database? Let's create a new module in the domain.mutations package of the related Django app, that will expose one single \"kwargs-only\" function - its name will start with a verb. Is it code that reads data from a database? Let's create a new module in the domain.queries package of the related Django app, that will expose one single \"kwargs-only\" function. Is it code that expresses the business logic but neither alters nor reads data from a database? Let's put that in a module of the domain package of the related Django app. The beauty of that pattern is that it really scales very well, despite its simplicity: my former teams and I used it for years on ever-growing code bases without having ever faced a case where the pattern would show a limitation. Info The 3rd (and last) article of this series will be a quick one, about how I hosted this app - for free - at the end of that single day of work. Acknowledgements \u00b6 Thank you so much HackSoft for your Django Styleguide ! I would also like to thank Audrey and Daniel Roy Greenfeld for their book Two Scoops Of Django , which was a very helpful resource for me when I started to learn Django and tried to see what the best practices could be in this ecosystem - definitely worth the purchase! And thanks again to my friend Yann - einenlum.com - for his careful review and useful feedback on this article Part 3 \u00b6 UPDATE: Part 3 is online","title":"'From scratch to online in production' in a single day, with Django - Part 2"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#from-scratch-to-online-in-production-in-a-single-day-with-django-part-2","text":"","title":"'From scratch to online in production' in a single day, with Django - Part 2"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#quick-summary-of-the-previous-part","text":"In part 1 we saw a way to set up a Django project in a quite generic way, that can be used for any kind of project: Dependencies are managed by Poetry Settings that differ from an environment to another come from environment variables, and are populated from optional .env files in \"local development\" mode, powered by django-environ . SECRET_KEY = env . str ( \"SECRET_KEY\" ) DATABASES = { \"default\" : env . db_url ( \"DATABASE_URL\" ), } On top of this we also have a Python module for each type of environment - so we'll use the DJANGO_SETTINGS_MODULE environment variable to point to project.settings.development during local development , project.settings.heroku on Heroku, project.settings.test when running our test suite, etc. Info Having such multiple settings files is a best practice I learned by reading the great book Two Scoops of Django - which itself cites this talk from Jacob Kaplan-Moss. A src/ folder contains all the application code, within 2 top-level packages for our Python modules: project , which contains the Django settings and the WSGI/ASGI HTTP entrypoints apps for the Django apps - so each of them is free to have any name we want, without any risks of collisions with an existing 3rd-party package.","title":"Quick summary of the previous part"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#next-step-we-have-to-organise-the-business-logic-inside-our-django-apps","text":"Django has some built-in recommendations about how to structure a project. Its applications concept for example is a good start to split the code into smaller units, and some basic rules like this one are good guidelines: Quote If there are 20+ models in a single app, think about ways to break it down into smaller apps, as it probably means your app is doing too much. In practice, we like to lower this number to no more than five to ten models per app . From Two Scoops of Django Now, let's take a look at an aspect that may be a mystery for other developers who enter this new ecosystem they're not familiar with yet - like me a few years ago, when I switched from \"PHP + Symfony \" to \"Python + Django \" : How to organise the business logic inside each Django app? Me, as I was learning Django Well, I guess \"the Django way\" would probably be to follow the Active Record pattern and write most of the code for this in the Models themselves - as well as in their Managers (which are similar to \"repositories\" or \"entity managers\" when using the Data Mapper pattern) However, in my own (subjective) case I generally find that doing so doesn't scale very well as the project grows, as we end up having all the business logic grouped in a bunch of huge classes. Which is why, as I was learning Django, I was quite eager to find another way to structure my code...","title":"Next step: we have to organise the business logic inside our Django apps"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#an-efficient-pattern-for-the-business-logic-simplicity-that-scales-well","text":"When I started to learn Django I was lucky enough to stumble upon this Django Styleguide , published on GitHub by the software development company HackSoft. And more specifically this part, where they explain \"services\" and \"selectors\": https://github.com/HackSoftware/Django-Styleguide#services","title":"An efficient pattern for the business logic: simplicity that scales well"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#my-own-adaptation-of-the-services-and-selectors-concept-from-hacksoft","text":"As I'm mostly working with GraphQL in my day-to-day job, I opted for a terminology that rings a bell a bit more to my ears than \"services\" and \"selectors\" : mutations and queries . The former is a package that contains code that alter a database (adding, modifying or deleting data from it), while the latter is specialised in fetching data. Here is how it looks like applied to my \"Gin Rummy leaderboard\" mini project: (the parts of the tree that don't matter in this case are replaced with three dots) gin-scoring/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 apps/ \u2502 \u2502 \u251c\u2500\u2500 authentication/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 gin_scoring/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 mutations/ # (1) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _save_game_result.py \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 queries/ # (2) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame_monthly.py \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 _hall_of_fame.py \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 _last_game_results.py \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 gin_rummy.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 jinja2/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 migrations/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 admin.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 apps.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 helpers.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 http_payloads.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 views.py \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 project/ \u2502 \u2502 \u251c\u2500\u2500 settings/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u251c\u2500\u2500 asgi.py # (3) \u2502 \u2502 \u251c\u2500\u2500 jinja2.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u2514\u2500\u2500 manage.py* \u251c\u2500\u2500 tests/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml In this package we have the code that alters data In this package we have the code that fetches data asgi.py , urls.py and wsgi.py were created by the startproject command. jinja2.py is where I quickly configure Jinja, following the Django documentation: > https://docs.djangoproject.com/en/4.0/topics/templates/#django.template.backends.jinja2.Jinja2 We can see that for each Django app we have 2 sub-packages: domain.mutations is where the code that alter data lives domain.queries is where the code that fetch data lives They're both structured the same way, following these principles from HackSoft's styleguide: Each of their module exposes only one public function - and optionally some types if it has to, in order to describe the shape of its input and/or output. Each of these one-per-module-functions only accepts keyword arguments. These functions' signatures should be fully \"type hinted\". Each module is free to use as many private functions it needs to achieve the job described by its single public function. For the mutations, these functions' name should start with a verb , since they're the reflection of a business logic action Each module's name is prefixed with an underscore, to emphasise that it should not be imported directly The __init__.py file is in charge of exposing the public function of each module to the \"outside world\" - i.e. the Python code that doesn't live in the same package. Info The goal of the last 2 points is to avoid this kind of imports, where we have to repeat the same name twice - once of the module name and once for the function itself: from .domain.mutations.save_game_result import save_game_result So instead of this repetition we can simply do: from .domain.mutations import save_game_result","title":"My own adaptation of the \"services\" and \"selectors\" concept from HackSoft"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#a-concrete-example-with-the-gin-rummy-leaderboard-app","text":"Ok, enough theory - let's see how that works in the context of this mini project! We have a single Django model , that looks like this: class GameResult ( models . Model ): player_north_name = models . CharField ( max_length = 50 ) player_south_name = models . CharField ( max_length = 50 ) outcome = models . CharField ( max_length = 10 , choices = [( outcome , outcome ) for outcome in GAME_OUTCOME . __args__ ]) # type: ignore # These 2 ones can be `null` when the outcome is `draw`: winner_name = models . CharField ( max_length = 50 , null = True ) deadwood_value = models . PositiveSmallIntegerField ( null = True ) # Computed from the previous `outcome` and `deadwood_value` fields: winner_score = models . PositiveSmallIntegerField ( null = True ) created_at = models . DateTimeField ( default = timezone . now ) @property def is_draw ( self ) -> bool : return self . outcome == \"draw\" @cached_property def loser_name ( self ) -> str | None : if self . is_draw : return None return [ name for name in ( self . player_north_name , self . player_south_name ) if name != self . winner_name ][ 0 ] def __str__ ( self ) -> str : return f \" { self . player_north_name . title () } vs { self . player_south_name . title () } , on { self . created_at . strftime ( ' %a %d %b at %H:%M' ) } \"","title":"A concrete example, with the Gin Rummy leaderboard app"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#the-domainmutations-package-of-our-django-app","text":"For this minimalist project we need only one mutation, which is triggered when the user submits the \"New game result\" HTML form: # file: src/apps/gin_scoring/domain/_save_game_result.py from ...domain.gin_rummy import GAME_OUTCOME , calculate_round_score from ...models import GameResult def save_game_result ( * , # (1) player_north_name : str , player_south_name : str , outcome : GAME_OUTCOME , # (2) winner_name : str | None , deadwood_value : int , ) -> GameResult : is_draw = outcome == \"draw\" winner_score = None if is_draw : winner_name = None else : winner_score = calculate_round_score ( game_outcome = outcome , deadwood_value = deadwood_value ) game_result_model = GameResult ( # (3) player_north_name = player_north_name , player_south_name = player_south_name , outcome = outcome , winner_name = winner_name , deadwood_value = deadwood_value , winner_score = winner_score , ) # (4) game_result_model . save () return game_result_model We force this function to be used only with the \"keyword arguments\" syntax GAME_OUTCOME is just a literal type, described later on in this same article To my knowledge there's no equivalent of the Shorthand property names of ES2015 in Python - which I guess must be on purpose, since readability is almost always the top priority of the language? For that reason we have to repeat the [field name]=[arg name] pattern, but in my opinion it's not really an issue The model also has a created_at field, but it will automatically be set by the Django ORM since we've used default=timezone.now when we defined the models.DateTimeField field And then, we expose that function to the rest of the Python code: # file: src/apps/gin_scoring/domain/mutations/__init__.py from ._save_game_result import save_game_result All we have to do now is to use that mutation from a Django view. There are several ways to do this, but here is an example: # file: src/apps/gin_scoring/views.py from .domain.mutations import save_game_result # ... @require_POST def post_game_result ( request : HttpRequest ) -> HttpResponse : try : game_result_payload = GameResultPayload ( ** request . POST . dict ()) except pydantic . ValidationError : return HttpResponseBadRequest () save_game_result ( # (1) player_north_name = game_result_payload . player_north_name , player_south_name = game_result_payload . player_south_name , outcome = game_result_payload . outcome , winner_name = game_result_payload . winner_name , deadwood_value = game_result_payload . deadwood_value , ) return redirect ( \"index\" ) Note that we could also have opted for a more concise way to transfer data from the \"validation and normalisation\" data structure to the mutation: save_game_result ( ** game_result_payload . dict () ) Validating the input of our Django views There are multiple ways to validate and normalise the input of our Django views, before passing its data to the \"domain\" layer. In this case I chose to use Pydantic . # file: src/apps/gin_scoring/http_payloads.py from typing import Any import pydantic from .domain.gin_rummy import GAME_OUTCOME from .helpers import normalize_player_name class GameResultPayload ( pydantic . BaseModel ): player_north_name : str player_south_name : str outcome : GAME_OUTCOME winner_name : str | None deadwood_value : int @pydantic . root_validator ( pre = True ) def normalize_player_names ( cls , values : dict [ str , Any ]): # In order to have consistent recording when players \"Rachel\" and \"Olivier\" add a game result, whether # \"Rachel\" is \"north\" and \"Olivier\" is \"south\" or vice-versa, we sort their names alphabetically # and then always set the \"north\" player to the first one and the \"south\" one to the second one: player_north_name , player_south_name = sorted ( ( normalize_player_name ( values [ \"player_north_name\" ]), normalize_player_name ( values [ \"player_south_name\" ]), ) ) values [ \"player_north_name\" ] = player_north_name values [ \"player_south_name\" ] = player_south_name return values @pydantic . validator ( \"winner_name\" ) def validate_winner_name ( cls , v : str , values : dict [ str , Any ]) -> str | None : is_draw = values [ \"outcome\" ] == \"draw\" if is_draw : return None # No winner name for \"draw\" games if not v : raise ValueError ( f \"non-draw games must have a winner name\" ) winner_name = normalize_player_name ( v ) player_names = ( values [ \"player_north_name\" ], values [ \"player_south_name\" ]) if winner_name not in player_names : raise ValueError ( f \"winner name { v } is not part of the players' names ' { ',' . join ( player_names ) } '\" ) return winner_name","title":"The domain.mutations package of our Django app"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#the-domainqueries-package-of-our-django-app","text":"For this simple app we need only 3 queries: One to get the global \"Hall of fame\", where we determine the ranking of players based on all the games played so far One to get the monthly \"Hall of fame\", which does the same but with a distinct ranking for each month One that returns exhaustive data for the last 10 games that were played Let's take a look at the second one, for example: # file: src/apps/gin_scoring/domain/queries/_hall_of_fame_monthly.py from collections import defaultdict from datetime import datetime from typing import NamedTuple from django.db.models import Count , Sum from django.db.models.functions import TruncMonth from ...models import GameResult # (1) class HallOfFameMonthResult ( NamedTuple ): # (2) month : datetime winner_name : str game_counts : int win_counts : int score_delta : int def hall_of_fame_monthly () -> list [ HallOfFameMonthResult ]: # \u26a0\ufe0f Probably not the very best way to achieve this... # But this is a project I gave myself one single day to build, # so that will do the job \ud83d\ude05 # @link https://docs.djangoproject.com/en/4.0/topics/db/aggregation/ win_counts = Count ( \"winner_score\" ) total_score = Sum ( \"winner_score\" ) raw_results = ( GameResult . objects . filter ( winner_name__isnull = False ) . annotate ( month = TruncMonth ( \"created_at\" )) # (3) . values ( \"month\" , \"winner_name\" ) . distinct () . annotate ( win_counts = win_counts , total_score = total_score ) # Each won round is worth 25 points: . annotate ( grand_total = ( win_counts * 25 ) + total_score ) . order_by ( \"-month\" , \"-grand_total\" ) ) raw_results_per_month : dict [ datetime , list [ dict ]] = defaultdict ( list ) for raw_result in raw_results : raw_results_per_month [ raw_result [ \"month\" ]] . append ( raw_result ) returned_results : list [ HallOfFameMonthResult ] = [] for month , month_results in raw_results_per_month . items (): winner_result = month_results [ 0 ] winner_grand_total = winner_result [ \"grand_total\" ] or 0 second_best_grand_total = 0 if len ( month_results ) < 2 else ( month_results [ 1 ][ \"grand_total\" ] or 0 ) games_count = sum ([ res [ \"win_counts\" ] for res in month_results ]) returned_results . append ( HallOfFameMonthResult ( month = month , winner_name = winner_result [ \"winner_name\" ], game_counts = games_count , win_counts = winner_result [ \"win_counts\" ], score_delta = winner_grand_total - second_best_grand_total , ) ) return returned_results I like using relative imports for things we import that live in the same Django app . In all other cases I'd use absolute imports. We're not returning ActiveRecord items directly from a Django QuerySet there, but aggregated results. There are several data structures we can use in Python for that kind of \"value objects\", but I generally opt for typing.NamedTuple This is where we group the database rows by month Yes, I could probably have used itertools.groupby instead ^_^ Note Note that we could have split this into several functions - in which case they would all be private functions (their name would start with an underscore), and only the \"domain\" one would be public And similarly, the __init__.py file is in charge of exposing only what the rest of the Python # file: src/apps/gin_scoring/domain/queries/__init__.py from ._hall_of_fame import hall_of_fame from ._hall_of_fame_monthly import ( HallOfFameMonthResult , # (1) hall_of_fame_monthly ) from ._last_game_results import last_game_results # (2) Sometimes it's useful to not only export the one public function of the module, but also a dedicated type it's using for its input or output - so other Python modules can also use type hints when interacting with the \"domain\" layer But most of the time, all we need is to expose the public function of the module","title":"The domain.queries package of our Django app"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#but-what-about-the-domain-logic-that-is-neither-a-mutation-nor-a-query","text":"We still have to put somewhere some parts of the domain don't fall in either categories. For example: Constants, enums, literal types... Data structures describing some aspects of the business logic, that can be used by both mutations and queries. Various forms of \"memory-only\" computations. Well... In my case, I find that in each Django app the domain package itself is a very good place to welcome these! For this mini project, for example, I chose to have a single Python module ( apps.gin_scoring.domain.gin_rummy ) to store some \"business-logic-related\" stuff that is specific to the Gin Rummy game , and that are neither a mutation nor a query: # file: src/apps/gin_scoring/domain/gin_rummy.py from typing import Literal # Possible outcomes of a Gin Rummy game: # (1) GAME_OUTCOME = Literal [ \"knock\" , \"gin\" , \"big_gin\" , \"undercut\" , \"draw\" ] def calculate_round_score ( * , game_outcome : GAME_OUTCOME , deadwood_value : int ) -> int : # @link https://en.wikipedia.org/wiki/Gin_rummy#Knocking match game_outcome : case \"draw\" : return 0 case \"knock\" : return deadwood_value case \"gin\" : return 25 + deadwood_value case \"big_gin\" : return 31 + deadwood_value case \"undercut\" : return 15 + deadwood_value case _ : raise ValueError ( f \"Invalid game outcome value ' { game_outcome } '\" ) More on that Literal type below We can see indeed that calculating the score of a round, depending on its outcome and the value of its deadwood , is neither a mutation nor a query: it's just a standalone computation, that does not depend on anything we would have in a database. And the same goes for enumerating the possible outcomes of a Gin Rummy game. Literals or Enums? To express the outcome of a Gin Rummy game I could of course have used a Python enum instead: @enum . unique class GameOutcome ( enum . Enum ): KNOCK = \"knock\" # or `enum.auto()` GIN = \"gin\" BIG_GIN = \"big_gin\" UNDERCUT = \"undercut\" DRAW = \"draw\" As for me, I must admit that I have no strict rules when I have to choose between one or another way to describe that kind of data I was mostly using Enums until a few years ago, but as I was using TypeScript more and more I realised that I really liked using literal types there - the TypeScript equivalent of that Python Literal would be: export type GAME_OUTCOME = \"knock\" | \"gin\" | \"big_gin\" | \"undercut\" | \"draw\" I appreciate the concision of literals, and tend to use them when I have the feeling that having such \"literal values\" spread in the code wouldn't cause any issue later on if I have to change their values Enums are certainly easier to handle in case of refactorings, but so far I've never come across a case where during a refactoring I regretted having opted for a literal rather than an Enum - fingers crossed, it won't be the case anytime soon!","title":"But what about the domain logic that is neither a mutation nor a query?"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#and-thats-it","text":"As we can see the pattern is very simple to implement, and its few principles are a very good guideline for developers when they have to add some code. Is it code that creates, updates or deletes data in a database? Let's create a new module in the domain.mutations package of the related Django app, that will expose one single \"kwargs-only\" function - its name will start with a verb. Is it code that reads data from a database? Let's create a new module in the domain.queries package of the related Django app, that will expose one single \"kwargs-only\" function. Is it code that expresses the business logic but neither alters nor reads data from a database? Let's put that in a module of the domain package of the related Django app. The beauty of that pattern is that it really scales very well, despite its simplicity: my former teams and I used it for years on ever-growing code bases without having ever faced a case where the pattern would show a limitation. Info The 3rd (and last) article of this series will be a quick one, about how I hosted this app - for free - at the end of that single day of work.","title":"And that's it!"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#acknowledgements","text":"Thank you so much HackSoft for your Django Styleguide ! I would also like to thank Audrey and Daniel Roy Greenfeld for their book Two Scoops Of Django , which was a very helpful resource for me when I started to learn Django and tried to see what the best practices could be in this ecosystem - definitely worth the purchase! And thanks again to my friend Yann - einenlum.com - for his careful review and useful feedback on this article","title":"Acknowledgements"},{"location":"2022/07-22---from-scratch-to-online-in-production-in-a-single-day-with-django-part-2/#part-3","text":"UPDATE: Part 3 is online","title":"Part 3"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/","tags":["TIL","github"],"text":"Triggering a GitHub Action from an external source \u00b6 Abstract TIL : a GitHub Action on a given repository can trigger Actions on other GitHub repositories - which is really handy, and enables fancy scenarios of cooperation between repositories! Setting up a self-updating GitHub profile, following the steps shared by Simon Willison \u00b6 Yesterday I've set up a custom GitHub profile, and that README file includes a \"code-generated\" part that automatically displays the last entries from this devblog. It was fun! I've followed the instructions from GitHub themselves, as well as the content generously shared by Simon Willison on his blog about explaining how his own (shiny ) GitHub profile is automatically updated once an hour with quite a lot of dynamic content: https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/managing-your-profile-readme https://simonwillison.net/2020/Jul/10/self-updating-profile-readme/ So far so good, by copy-pasting most of his code and adapting it to my own case I got my own GitHub profile also updated automatically once an hour. Note The main difference between Simon Willison's build script and mine is that the dynamic content I have in my own README is much smaller, as I only want to display the last 10 items of this devblog there . As a result the process can be simpler - and even managed only with Python and its standard library! Using the standard library to fetch the RSS feed of this devblog over HTTP and then parse it is a bit less straightforward than what I could have done with higher level packages like Requests and feedparser , but it's still pretty simple The main drawback is probably that Python's xml.etree.ElementTree has a big red warning saying it is \"not secure against maliciously constructed data\" - but in my case what I parse is the RSS feed from my own blog, so it shouldn't be a problem My own \"README update\" script is there: https://github.com/olivierphi/olivierphi/blob/main/build_readme.py Updating the GitHub profile only when the devblog is updated \u00b6 My friend Yann noticed that triggering this README generation once an hour doesn't really make sense, since contrary to Simon Willison the only source of dynamic data in this profile is the devblog: so what would be ideal would be to automatically update the README only when the devblog is updated . Two hours in the GitHub Actions documentation later, I got it working - let's keep a note of how to do this, so I can do it again later on more easily Triggering a GitHub Action from an external source \u00b6 It seems that the only way to trigger a GitHub Action from an event that is not on the GitHub repository itself is to use the repository_dispatch event . In my case the emitter of the event will be a GitHub Action living on this devblog repository, while the receiver will be another GitHub Action, living in the olivierphi repository. (since my GitHub profile is \"olivierphi\", according to the doc my GitHub profile must be a README file living in a GitHub repository that has the sane name) Of course, triggering a GitHub Action in a given repository must not be something that anyone can do, as it would be annoying to have such Actions triggered randomly by 3rd-party people or scripts - and even more annoying, running an Action can leak sensitive information Which is why the emitter of a repository_dispatch event have to authenticate itself . GitHub's Personal Access Tokens are a way to do this. Generating a GitHub Personal Access Token \u00b6 Unfortunately, it seems that Personal Access Tokens have quite a poor granularity: the emitter of a repository_dispatch event must use a token with the repo scope , which gives it \"full access to repositories, including private repositories\" Note I thought I could create such a Token that would be limited to my GitHub profile repository, but it seems that Personal Access Tokens don't have such a level of granularity, so I do have to give such a \"god-like\" access to my devblog repository The steps to generate such a token are documented here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token Storing the Token on the event emitter's side \u00b6 This Token having such great power, I should store it in a safe place... It seems that GitHub Actions' Encrypted Secrets is what I need! https://docs.github.com/en/actions/security-guides/encrypted-secrets Sending the repository_dispatch event when this DevBlog is updated \u00b6 Right, now I have a (overpowered ) Token, and it's safely stored in the devblog repository as an Encrypted Secret... Now all I have to do is to use it to send such an event to the repo that hosts my GitHub profile! There are multiple ways to do this, and I went for the one that was looking the most straightforward to me. So at the end of the GitHub Action file that deploys the blog generated by Material for MKDocs when I push an update to the git repo, I just added the following step : - name : Notify my Github profile repo env : TOKEN : ${{ secrets.MY_TOKEN_SECRET_NAME }} # (1) run : # (2) |- curl \\ --silent \\ -X POST \\ -H \"Accept : application/vnd.github+json\" \\ -H \"Authorization : token ${TOKEN}\" \\ \"https://api.github.com/repos/olivierphi/olivierphi/dispatches\" \\ -d '{\"event_type\":\"devblog-gh-pages-pushed\",\"client_payload\":{\"wait_for_deployment\":true}}' We ask GitHub to extract the encrypted secret to an environment variable that I name \"TOKEN\" The target URL includes olivierphi/olivierphi : the first one is the name of my GitHub profile, while the second one is the name of the GitHub repository. They have to be identical in the case of the GitHub profile. I copy-pasted this curl command from there: https://docs.github.com/en/rest/repos/repos#create-a-repository-dispatch-event Receiving the repository_dispatch event on the GitHub profile repo \u00b6 I already have a GitHub Action file that is in charge of re-building dynamically the content of my GitHUb profile's README when I push some content. All I have to do now is to remove the hourly build, and subscribe to the repository_dispatch event: on : push : workflow_dispatch : # (1) # This is removed: schedule : - cron : '33 * * * *' # rebuilt once an hour at xx:33 # This is added: repository_dispatch : # triggered by my \"devblog\" repo when something is pushed on the GH Pages branch types : [ devblog-gh-pages-pushed ] Thanks to this workflow_dispatch we can also manually trigger the GitHub Action . Conditionally waiting for the DevBlog's deployment \u00b6 There is one last thing I have to manage: at the time when this GitHub Action is triggered, the DevBlog's static content was just pushed to the gh-pages branch by MKDocs, so the updated DevBlog is not online yet ! So just doing this is not enough, and we need to wait for the RSS feed to be up-to-date before re-generating the README of the GitHub profile... It seems that most of the time this deployment takes about 30 to 40 seconds, but sometimes it took a little more than a minute; right, let's wait for 90 seconds before reading the RSS feed, and it should do the job. However, I don't want to have this waiting time when I push an updated version of the README file itself, or when I trigger the GitHub Action manually! 15 browser tabs opened on the GH Actions documentation later, I found a solution - name : Wait for devblog deployment on GitHub Pages if : ${{ github.event_name == 'repository_dispatch' && github.event.client_payload.wait_for_deployment }} run : |- echo \"Deployment can take up to 1 minute, let's wait for 90 seconds\" sleep 90 - name : Update README run : |- python build_readme.py cat README.md So the Action will sleep for 90 seconds before running the Python script that fetches the blog's content and update the README accordingly, but only if : The Action was triggered by a \"repository_dispatch\" event. We can detect this with: github.event_name == 'repository_dispatch' The emitter sent a parameter - that I arbitrarily called wait_for_deployment - in its JSON payload, with a truthy value (which is what I did in the curl command earlier ). We can detect this with: github.event.client_payload.wait_for_deployment Job's done! \u00b6 Now when I push to this DevBlog repo it sends a repository_dispatch event to the repository of my GitHub profile, which will trigger the generation of the README after having waited a bit to let some time for the up-to-date RSS feed to be online In the end, all this happens between these 2 YAML files: https://github.com/olivierphi/olivierphi/blob/main/.github/workflows/build.yml https://github.com/olivierphi/devblog/blob/main/.github/workflows/ci.yml","title":"Triggering a GitHub Action from an external source"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#triggering-a-github-action-from-an-external-source","text":"Abstract TIL : a GitHub Action on a given repository can trigger Actions on other GitHub repositories - which is really handy, and enables fancy scenarios of cooperation between repositories!","title":"Triggering a GitHub Action from an external source"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#setting-up-a-self-updating-github-profile-following-the-steps-shared-by-simon-willison","text":"Yesterday I've set up a custom GitHub profile, and that README file includes a \"code-generated\" part that automatically displays the last entries from this devblog. It was fun! I've followed the instructions from GitHub themselves, as well as the content generously shared by Simon Willison on his blog about explaining how his own (shiny ) GitHub profile is automatically updated once an hour with quite a lot of dynamic content: https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/managing-your-profile-readme https://simonwillison.net/2020/Jul/10/self-updating-profile-readme/ So far so good, by copy-pasting most of his code and adapting it to my own case I got my own GitHub profile also updated automatically once an hour. Note The main difference between Simon Willison's build script and mine is that the dynamic content I have in my own README is much smaller, as I only want to display the last 10 items of this devblog there . As a result the process can be simpler - and even managed only with Python and its standard library! Using the standard library to fetch the RSS feed of this devblog over HTTP and then parse it is a bit less straightforward than what I could have done with higher level packages like Requests and feedparser , but it's still pretty simple The main drawback is probably that Python's xml.etree.ElementTree has a big red warning saying it is \"not secure against maliciously constructed data\" - but in my case what I parse is the RSS feed from my own blog, so it shouldn't be a problem My own \"README update\" script is there: https://github.com/olivierphi/olivierphi/blob/main/build_readme.py","title":"Setting up a self-updating GitHub profile, following the steps shared by Simon Willison"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#updating-the-github-profile-only-when-the-devblog-is-updated","text":"My friend Yann noticed that triggering this README generation once an hour doesn't really make sense, since contrary to Simon Willison the only source of dynamic data in this profile is the devblog: so what would be ideal would be to automatically update the README only when the devblog is updated . Two hours in the GitHub Actions documentation later, I got it working - let's keep a note of how to do this, so I can do it again later on more easily","title":"Updating the GitHub profile only when the devblog is updated"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#triggering-a-github-action-from-an-external-source_1","text":"It seems that the only way to trigger a GitHub Action from an event that is not on the GitHub repository itself is to use the repository_dispatch event . In my case the emitter of the event will be a GitHub Action living on this devblog repository, while the receiver will be another GitHub Action, living in the olivierphi repository. (since my GitHub profile is \"olivierphi\", according to the doc my GitHub profile must be a README file living in a GitHub repository that has the sane name) Of course, triggering a GitHub Action in a given repository must not be something that anyone can do, as it would be annoying to have such Actions triggered randomly by 3rd-party people or scripts - and even more annoying, running an Action can leak sensitive information Which is why the emitter of a repository_dispatch event have to authenticate itself . GitHub's Personal Access Tokens are a way to do this.","title":"Triggering a GitHub Action from an external source"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#generating-a-github-personal-access-token","text":"Unfortunately, it seems that Personal Access Tokens have quite a poor granularity: the emitter of a repository_dispatch event must use a token with the repo scope , which gives it \"full access to repositories, including private repositories\" Note I thought I could create such a Token that would be limited to my GitHub profile repository, but it seems that Personal Access Tokens don't have such a level of granularity, so I do have to give such a \"god-like\" access to my devblog repository The steps to generate such a token are documented here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token","title":"Generating a GitHub Personal Access Token"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#storing-the-token-on-the-event-emitters-side","text":"This Token having such great power, I should store it in a safe place... It seems that GitHub Actions' Encrypted Secrets is what I need! https://docs.github.com/en/actions/security-guides/encrypted-secrets","title":"Storing the Token on the event emitter's side"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#sending-the-repository_dispatch-event-when-this-devblog-is-updated","text":"Right, now I have a (overpowered ) Token, and it's safely stored in the devblog repository as an Encrypted Secret... Now all I have to do is to use it to send such an event to the repo that hosts my GitHub profile! There are multiple ways to do this, and I went for the one that was looking the most straightforward to me. So at the end of the GitHub Action file that deploys the blog generated by Material for MKDocs when I push an update to the git repo, I just added the following step : - name : Notify my Github profile repo env : TOKEN : ${{ secrets.MY_TOKEN_SECRET_NAME }} # (1) run : # (2) |- curl \\ --silent \\ -X POST \\ -H \"Accept : application/vnd.github+json\" \\ -H \"Authorization : token ${TOKEN}\" \\ \"https://api.github.com/repos/olivierphi/olivierphi/dispatches\" \\ -d '{\"event_type\":\"devblog-gh-pages-pushed\",\"client_payload\":{\"wait_for_deployment\":true}}' We ask GitHub to extract the encrypted secret to an environment variable that I name \"TOKEN\" The target URL includes olivierphi/olivierphi : the first one is the name of my GitHub profile, while the second one is the name of the GitHub repository. They have to be identical in the case of the GitHub profile. I copy-pasted this curl command from there: https://docs.github.com/en/rest/repos/repos#create-a-repository-dispatch-event","title":"Sending the repository_dispatch event when this DevBlog is updated"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#receiving-the-repository_dispatch-event-on-the-github-profile-repo","text":"I already have a GitHub Action file that is in charge of re-building dynamically the content of my GitHUb profile's README when I push some content. All I have to do now is to remove the hourly build, and subscribe to the repository_dispatch event: on : push : workflow_dispatch : # (1) # This is removed: schedule : - cron : '33 * * * *' # rebuilt once an hour at xx:33 # This is added: repository_dispatch : # triggered by my \"devblog\" repo when something is pushed on the GH Pages branch types : [ devblog-gh-pages-pushed ] Thanks to this workflow_dispatch we can also manually trigger the GitHub Action .","title":"Receiving the repository_dispatch event on the GitHub profile repo"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#conditionally-waiting-for-the-devblogs-deployment","text":"There is one last thing I have to manage: at the time when this GitHub Action is triggered, the DevBlog's static content was just pushed to the gh-pages branch by MKDocs, so the updated DevBlog is not online yet ! So just doing this is not enough, and we need to wait for the RSS feed to be up-to-date before re-generating the README of the GitHub profile... It seems that most of the time this deployment takes about 30 to 40 seconds, but sometimes it took a little more than a minute; right, let's wait for 90 seconds before reading the RSS feed, and it should do the job. However, I don't want to have this waiting time when I push an updated version of the README file itself, or when I trigger the GitHub Action manually! 15 browser tabs opened on the GH Actions documentation later, I found a solution - name : Wait for devblog deployment on GitHub Pages if : ${{ github.event_name == 'repository_dispatch' && github.event.client_payload.wait_for_deployment }} run : |- echo \"Deployment can take up to 1 minute, let's wait for 90 seconds\" sleep 90 - name : Update README run : |- python build_readme.py cat README.md So the Action will sleep for 90 seconds before running the Python script that fetches the blog's content and update the README accordingly, but only if : The Action was triggered by a \"repository_dispatch\" event. We can detect this with: github.event_name == 'repository_dispatch' The emitter sent a parameter - that I arbitrarily called wait_for_deployment - in its JSON payload, with a truthy value (which is what I did in the curl command earlier ). We can detect this with: github.event.client_payload.wait_for_deployment","title":"Conditionally waiting for the DevBlog's deployment"},{"location":"2022/07-26---triggering-a-github-action-from-an-external-source/#jobs-done","text":"Now when I push to this DevBlog repo it sends a repository_dispatch event to the repository of my GitHub profile, which will trigger the generation of the README after having waited a bit to let some time for the up-to-date RSS feed to be online In the end, all this happens between these 2 YAML files: https://github.com/olivierphi/olivierphi/blob/main/.github/workflows/build.yml https://github.com/olivierphi/devblog/blob/main/.github/workflows/ci.yml","title":"Job's done!"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/","tags":["TIL","django","sqlite"],"text":"Making SQLite much faster in a local dev environment \u00b6 Abstract TIL : it is possible to make SQLite data insertions 3 times faster in a \"local development\" environment - where data integrity is not a crucial criteria. UPDATE : It turns out that we can even reach a \" 30 times faster \" gain with additional tweaks - see the update at the end of the article. Inserting data in a SQLite database can be quite slow \u00b6 A Quick \" TIL \" this time... Contrary to my \"Postgres\" habits, for one of my side projects I'm using SQLite quite heavily. Everything works fine, except that... Data insertions are very slow I suspected it could be caused by the extra work SQLite has to do to maintain data integrity, and started to check if it was possible to tune the level of strictness we want for this. SQLite has a \"journal mode\", and it can be customised \u00b6 That's how I found about the journal mode of SQLite: https://www.sqlite.org/pragma.html#pragma_journal_mode We can see on this documentation that it can be set to DELETE , TRUNCATE , PERSIST , MEMORY , WAL or OFF . Right, let's try to change that! At the beginning of the Python function that processes my big batch of data to insert, I've added a SQL query that - given that I understand that doc correctly - should make my data insertions faster: pragma journal_mode = memory ; Let's launch my data insertion script with this update! Result : nothing has changed, the insertion of my \u22482.000 rows still takes 50 seconds Ah, but maybe I need to send this query when the database connection is initiated, rather than on the fly when I need it? Note also that the journal_mode cannot be changed while a transaction is active. The SQLite documentation Setting the \"journal mode\" when the database connection is initialised \u00b6 As I'm using Django for this side project, I had to find a way to send a SQL query as soon as possible to the database, right after the connection is initialised. According to this (old) ticket on the Django bug tracker, which refers to that Stack Overflow thread , a way to do this is to plug a custom function to the signal that Django sends when the connection is initialised, and send the query there. Alright, let's try this! I want to lower the data integrity work only on my \"local dev\" environment, so I'll have to add this code to my project.settings.development module. (explanations about this module in this other post ) # file: src/project/settings/development.py # My existing settings... ... # ...to which I'm adding the following: # Setting SQLite journal mode to 'memory' - much faster writes, # at the expense of database safety and integrity. # @link https://www.sqlite.org/pragma.html#pragma_journal_mode # @link https://code.djangoproject.com/ticket/24018#comment:4 from django.db.backends.signals import connection_created def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory'\" ) cursor = connection . cursor () cursor . execute ( \"PRAGMA journal_mode = memory;\" ) connection_created . connect ( _disable_sqlite_journal ) Let's launch my data insertion script again! (x2) Result : the insertion of my \u22482.000 rows now takes 15 seconds, instead of 50 ! Closing notes \u00b6 The trade-off is clear: The MEMORY journaling mode stores the rollback journal in volatile RAM. This saves disk I/O but at the expense of database safety and integrity. If the application using SQLite crashes in the middle of a transaction when the MEMORY journaling mode is set, then the database file will very likely go corrupt. The SQLite documentation That's why in my case I want to customise this journal mode only on my local environment , and won't do it in production. But being able to opt in for 3 times faster data insertions is still a pretty good discovery , as being able to iterate quickly is crucial when working on such a local environment! UPDATE \u00b6 Following the publication of this article, the amazing Albin gave me a pointer to this URL: https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations If I activate all these settings, the original 50 seconds now become... 1.7 seconds! That's pretty much a \"30 times faster\" gain! from django.db.backends.signals import connection_created _UNLEASH_SQLITE_QUERIES = [ # @link https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations \"PRAGMA journal_mode = memory\" , \"PRAGMA synchronous = 0\" , \"PRAGMA cache_size = 1000000\" , \"PRAGMA locking_mode = EXCLUSIVE\" , \"PRAGMA temp_store = MEMORY\" , ] def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory', and various other settings\" ) cursor = connection . cursor () for sql in _UNLEASH_SQLITE_QUERIES : cursor . execute ( sql ) connection_created . connect ( _disable_sqlite_journal )","title":"Making SQLite much faster in a local dev environment"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#making-sqlite-much-faster-in-a-local-dev-environment","text":"Abstract TIL : it is possible to make SQLite data insertions 3 times faster in a \"local development\" environment - where data integrity is not a crucial criteria. UPDATE : It turns out that we can even reach a \" 30 times faster \" gain with additional tweaks - see the update at the end of the article.","title":"Making SQLite much faster in a local dev environment"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#inserting-data-in-a-sqlite-database-can-be-quite-slow","text":"A Quick \" TIL \" this time... Contrary to my \"Postgres\" habits, for one of my side projects I'm using SQLite quite heavily. Everything works fine, except that... Data insertions are very slow I suspected it could be caused by the extra work SQLite has to do to maintain data integrity, and started to check if it was possible to tune the level of strictness we want for this.","title":"Inserting data in a SQLite database can be quite slow"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#sqlite-has-a-journal-mode-and-it-can-be-customised","text":"That's how I found about the journal mode of SQLite: https://www.sqlite.org/pragma.html#pragma_journal_mode We can see on this documentation that it can be set to DELETE , TRUNCATE , PERSIST , MEMORY , WAL or OFF . Right, let's try to change that! At the beginning of the Python function that processes my big batch of data to insert, I've added a SQL query that - given that I understand that doc correctly - should make my data insertions faster: pragma journal_mode = memory ; Let's launch my data insertion script with this update! Result : nothing has changed, the insertion of my \u22482.000 rows still takes 50 seconds Ah, but maybe I need to send this query when the database connection is initiated, rather than on the fly when I need it? Note also that the journal_mode cannot be changed while a transaction is active. The SQLite documentation","title":"SQLite has a \"journal mode\", and it can be customised"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#setting-the-journal-mode-when-the-database-connection-is-initialised","text":"As I'm using Django for this side project, I had to find a way to send a SQL query as soon as possible to the database, right after the connection is initialised. According to this (old) ticket on the Django bug tracker, which refers to that Stack Overflow thread , a way to do this is to plug a custom function to the signal that Django sends when the connection is initialised, and send the query there. Alright, let's try this! I want to lower the data integrity work only on my \"local dev\" environment, so I'll have to add this code to my project.settings.development module. (explanations about this module in this other post ) # file: src/project/settings/development.py # My existing settings... ... # ...to which I'm adding the following: # Setting SQLite journal mode to 'memory' - much faster writes, # at the expense of database safety and integrity. # @link https://www.sqlite.org/pragma.html#pragma_journal_mode # @link https://code.djangoproject.com/ticket/24018#comment:4 from django.db.backends.signals import connection_created def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory'\" ) cursor = connection . cursor () cursor . execute ( \"PRAGMA journal_mode = memory;\" ) connection_created . connect ( _disable_sqlite_journal ) Let's launch my data insertion script again! (x2) Result : the insertion of my \u22482.000 rows now takes 15 seconds, instead of 50 !","title":"Setting the \"journal mode\" when the database connection is initialised"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#closing-notes","text":"The trade-off is clear: The MEMORY journaling mode stores the rollback journal in volatile RAM. This saves disk I/O but at the expense of database safety and integrity. If the application using SQLite crashes in the middle of a transaction when the MEMORY journaling mode is set, then the database file will very likely go corrupt. The SQLite documentation That's why in my case I want to customise this journal mode only on my local environment , and won't do it in production. But being able to opt in for 3 times faster data insertions is still a pretty good discovery , as being able to iterate quickly is crucial when working on such a local environment!","title":"Closing notes"},{"location":"2022/07-28---making-sqlite-much-faster-in-a-local-dev-environment/#update","text":"Following the publication of this article, the amazing Albin gave me a pointer to this URL: https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations If I activate all these settings, the original 50 seconds now become... 1.7 seconds! That's pretty much a \"30 times faster\" gain! from django.db.backends.signals import connection_created _UNLEASH_SQLITE_QUERIES = [ # @link https://avi.im/blag/2021/fast-sqlite-inserts/#sqlite-optimisations \"PRAGMA journal_mode = memory\" , \"PRAGMA synchronous = 0\" , \"PRAGMA cache_size = 1000000\" , \"PRAGMA locking_mode = EXCLUSIVE\" , \"PRAGMA temp_store = MEMORY\" , ] def _disable_sqlite_journal ( sender , connection , ** kwargs ): import logging if connection . vendor == \"sqlite\" : logging . getLogger ( \"apps\" ) . warning ( \"Setting SQLite journal mode to 'memory', and various other settings\" ) cursor = connection . cursor () for sql in _UNLEASH_SQLITE_QUERIES : cursor . execute ( sql ) connection_created . connect ( _disable_sqlite_journal )","title":"UPDATE"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/","tags":["django","deployment"],"text":"'From scratch to online in production' in a single day, with Django - Part 3 \u00b6 Prototype is working, now it's time to deploy it \u00b6 In part 1 we saw a way to set up a Django project in a quite generic way, that can be used for any kind of project. In part 2 I explained a bit how I structure my Django projects, based on HackSoft's Django Styleguide with just a slight personal deviation on top of it. After this I quickly set up an HTML/CSS user interface, rendered by Jinja templates and powered by the Bulma CSS framework - which is quite good for rapid prototyping. After a few quick tests I had a working prototype around 4pm , for this project started the same day at 9am. Which means I had about one more hour to deploy it to production to meet my goal! *rolling up his sleeves for the home straight Deploying a Django app to Heroku, with a free plan \u00b6 Heroku no longer has the great reputation it used to have back in the days, but I still like it and tend to still choose it as my primary deployment target in my commercial projects, as it still comes with some really strong benefits: Deploying a project is just a matter of doing git push to the Heroku git remote Rolling back to a previous version is just a click on their dashboard - or a command with their CLI client Postgres and Redis are builtin and fully managed Review Apps are still pretty amazing : a push to a branch (or the creation of a PR) automatically spawns a new environment for this branch, with its own database - that will be destroyed automatically when we're no longer using it. This really is a critical feature to me, as it allow project managers to test a new feature or a bugfix before it's merged, without having to spend days and days setting up and maintaining such a process manually. It's possible to deploy Docker images, but one can also rely on open source buildpacks , which are basically pre-configured setup scripts that install the software we need for a given stack. There are buildpacks for Python, Node.js, Ruby, Go, PHP, Java... Buildpacks can have framework-specific steps, such as running the collectstatic Django command automatically if Django is detected in the project However, the fact that I'm using Poetry to manage my Python dependencies is an edge case that the Python buildpack doesn't handle, so we have to use a trick to make this work Deploying a Poetry-powered Python project to Heroku \u00b6 A few years ago I had the need to add some custom steps to the deployment of a Django project, and by inspecting the source code of the Python buildpack I found this in the compilation script : # Experimental post_compile hook. Don't remove this. source \" $BIN_DIR /steps/hooks/post_compile\" ...which in turns triggers this script : if [ -f bin/post_compile ] ; then echo \"-----> Running post-compile hook\" chmod +x bin/post_compile sub_env bin/post_compile fi Right, so if there is a bin/post_compile file in the git repository it will be executed by Heroku after its own \"compilation\" step... That sounds like a good target for my custom deployment steps! My bin/post_compile script \u00b6 Here is the quick Bash script I made a few years ago, and that I've been copy-pasting from project to project since when I deploy them to Heroku: #!/bin/bash # file: /bin/post_compile # Heroku \"hidden\" post-compilation hook # (had to dig into the Heroku Python build pack source code to find that :-) set -eo pipefail echo '**** CUSTOM HEROKU PYTHON BUILD PACK \"bin/post_compile\" HOOK' indent () { sed \"s/^/ /\" } puts-step () { echo \"-----> $@ \" } puts-step \"Installing dependencies with Poetry...\" poetry config virtualenvs.create false 2 > & 1 | indent poetry install --no-dev 2 > & 1 | indent puts-step \"Collecting static files, now that Whitenoise is installed...\" python src/manage.py collectstatic --no-input 2 > & 1 | indent # Any other custom step can go here :-) So in this script I simply add a custom step where I install my dependencies with Poetry, and once it's done I trigger the collectstatic Django command. It's easy then to add other custom steps to this script, depending on the project. To get this work we only need to do these 2 things beforehand: Still have a root requirements.txt file - so that Heroku automatically recognises a Python project - and specify one single dependency there: Poetry itself. # file: /requirements.txt # This is only required to satisfy Heroku's Python build pack, # which doesn't handle Poetry (yet?). # The real dependencies install will happen in the \"bin/post_compile\" file. poetry == 1 .1.13 Ask the buildpack to not try to run collectstatic itself in its own compilation script: in production we serve our static assets with WhiteNoise , but this package is installed with Poetry later on in our bin/post_compile script and would not be found at the time when the builtin compilation script is triggered. Thankfully, the buildpack's documentation explains how to opt-out from this compilation step , simply by setting the DISABLE_COLLECTSTATIC environment variable. Which can be made via the Web dashboard or the Heroku CLI: heroku config:set DISABLE_COLLECTSTATIC = 1 Limitations of the Heroku free plan \u00b6 As every free plan, Heroku's one come with some limitations. The Postgres database is limited to 10,000 max rows, 1GB max storage and 20 connections at a time (details here ). For this \"Gin Rummy leaderboard\" these limits are not a problem The app itself is put into \"sleep\" after 30 minutes of inactivity, and waking it up typically takes a few seconds. That's obviously a very annoying restriction for \"real\" projects However, in this case this is just a simple Web page that will only record the Gin Rummy scores of the games I play with my partner, so having to wait a few sec for recording the first score when we start to play a few rounds is not a big deal. With real commercial projects I tend to start with the free plan, so project managers can start using the project as soon as possible - and we migrate to paid plans only when we need to. Using the app \u00b6 By picking Heroku, a platform I'm familiar with, I was able to have this \"Gin Rummy leaderboard\" Django app online in production around 5pm; we started to record our first scores there an hour later when we met at the pub. I typed django-admin startproject at 9am, and used the project in production at 6pm - challenge accomplished! ...and it was really fun to give myself this constraint Addendum: deployment to Fly.io \u00b6 Out of curiosity, a few days later I also implemented the deployment of the Django project to Fly.io - which is a platform I wanted to try for a while. I just had to create a Dockerfile for my app, and then follow their documentation to deploy my Docker image to their infrastructure and link a Postgres database to it. Despite the fact that it was the very first time I was using it, one hour later I had my instance running - which tells how straightforward deploying an app to Fly.io is! My quick fly.toml file looks like this: app = \"[the name of my app]\" kill_signal = \"SIGINT\" kill_timeout = 5 processes = [] [env] PORT = \"8080\" DJANGO_SETTINGS_MODULE = \"project.settings.flyio\" [[services]] internal_port = 8080 processes = [ \"app\" ] protocol = \"tcp\" script_checks = [] [[services.ports]] force_https = true handlers = [ \"http\" ] port = 80 [[services.ports]] handlers = [ \"tls\" , \"http\" ] port = 443 [[services.http_checks]] interval = 15000 grace_period = \"5s\" method = \"get\" path = \"/\" protocol = \"https\" restart_limit = 0 timeout = 1000 tls_skip_verify = false [[statics]] guest_path = \"/app/staticfiles\" url_prefix = \"/static/\" And contrary to Heroku, the app doesn't sleep after 30 minutes of inactivity. The only downside is that although I'm using their free allowance I had to give my credit card detail to Fly.io before being able to deploy my first app - which is not the case with Heroku. My quick \"Django project\" Dockerfile \u00b6 There are plenty of resources on the Web to create fine-tuned Dockerfiles for a Django project, but here is the one I quickly set up to deploy this side project to Fly.io: ARG PYTHON_VERSION = 3 .10 FROM python:${PYTHON_VERSION} AS build ENV DEBIAN_FRONTEND = noninteractive ENV PYTHONDONTWRITEBYTECODE = 1 PYTHONUNBUFFERED = 1 RUN apt-get update && apt-get install -y \\ python3-pip \\ python3-venv \\ python3-dev \\ python3-setuptools \\ python3-wheel \\ libpq-dev \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN pip install poetry RUN mkdir -p /app WORKDIR /app RUN python -m venv .venv COPY pyproject.toml poetry.lock ./ RUN poetry install --no-dev FROM python:${PYTHON_VERSION}-slim AS run ENV DEBIAN_FRONTEND = noninteractive RUN apt-get update && apt-get install -y \\ libpq5 \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN mkdir -p /app WORKDIR /app RUN addgroup -gid 1001 webapp RUN useradd --gid 1001 --uid 1001 webapp RUN chown -R 1001 :1001 /app USER 1001:1001 COPY --chown = 1001 :1001 --from = build /app/.venv .venv COPY --chown = 1001 :1001 . . ENV PYTHONPATH = /app/src RUN SECRET_KEY = does-not-matter-for-this-command DATABASE_URL = sqlite://:memory: ALLOWED_HOSTS = fly.io \\ .venv/bin/python src/manage.py collectstatic --noinput EXPOSE 8080 CMD [ \".venv/bin/gunicorn\" , \"--bind\" , \":8080\" , \"--workers\" , \"2\" , \"project.wsgi\" ] There is of course room for a lot of improvement there, but this quick multi-stage Dockerfile creates a 220MB image which is not too bad","title":"'From scratch to online in production' in a single day, with Django - Part 3"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#from-scratch-to-online-in-production-in-a-single-day-with-django-part-3","text":"","title":"'From scratch to online in production' in a single day, with Django - Part 3"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#prototype-is-working-now-its-time-to-deploy-it","text":"In part 1 we saw a way to set up a Django project in a quite generic way, that can be used for any kind of project. In part 2 I explained a bit how I structure my Django projects, based on HackSoft's Django Styleguide with just a slight personal deviation on top of it. After this I quickly set up an HTML/CSS user interface, rendered by Jinja templates and powered by the Bulma CSS framework - which is quite good for rapid prototyping. After a few quick tests I had a working prototype around 4pm , for this project started the same day at 9am. Which means I had about one more hour to deploy it to production to meet my goal! *rolling up his sleeves for the home straight","title":"Prototype is working, now it's time to deploy it"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#deploying-a-django-app-to-heroku-with-a-free-plan","text":"Heroku no longer has the great reputation it used to have back in the days, but I still like it and tend to still choose it as my primary deployment target in my commercial projects, as it still comes with some really strong benefits: Deploying a project is just a matter of doing git push to the Heroku git remote Rolling back to a previous version is just a click on their dashboard - or a command with their CLI client Postgres and Redis are builtin and fully managed Review Apps are still pretty amazing : a push to a branch (or the creation of a PR) automatically spawns a new environment for this branch, with its own database - that will be destroyed automatically when we're no longer using it. This really is a critical feature to me, as it allow project managers to test a new feature or a bugfix before it's merged, without having to spend days and days setting up and maintaining such a process manually. It's possible to deploy Docker images, but one can also rely on open source buildpacks , which are basically pre-configured setup scripts that install the software we need for a given stack. There are buildpacks for Python, Node.js, Ruby, Go, PHP, Java... Buildpacks can have framework-specific steps, such as running the collectstatic Django command automatically if Django is detected in the project However, the fact that I'm using Poetry to manage my Python dependencies is an edge case that the Python buildpack doesn't handle, so we have to use a trick to make this work","title":"Deploying a Django app to Heroku, with a free plan"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#deploying-a-poetry-powered-python-project-to-heroku","text":"A few years ago I had the need to add some custom steps to the deployment of a Django project, and by inspecting the source code of the Python buildpack I found this in the compilation script : # Experimental post_compile hook. Don't remove this. source \" $BIN_DIR /steps/hooks/post_compile\" ...which in turns triggers this script : if [ -f bin/post_compile ] ; then echo \"-----> Running post-compile hook\" chmod +x bin/post_compile sub_env bin/post_compile fi Right, so if there is a bin/post_compile file in the git repository it will be executed by Heroku after its own \"compilation\" step... That sounds like a good target for my custom deployment steps!","title":"Deploying a Poetry-powered Python project to Heroku"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#my-binpost_compile-script","text":"Here is the quick Bash script I made a few years ago, and that I've been copy-pasting from project to project since when I deploy them to Heroku: #!/bin/bash # file: /bin/post_compile # Heroku \"hidden\" post-compilation hook # (had to dig into the Heroku Python build pack source code to find that :-) set -eo pipefail echo '**** CUSTOM HEROKU PYTHON BUILD PACK \"bin/post_compile\" HOOK' indent () { sed \"s/^/ /\" } puts-step () { echo \"-----> $@ \" } puts-step \"Installing dependencies with Poetry...\" poetry config virtualenvs.create false 2 > & 1 | indent poetry install --no-dev 2 > & 1 | indent puts-step \"Collecting static files, now that Whitenoise is installed...\" python src/manage.py collectstatic --no-input 2 > & 1 | indent # Any other custom step can go here :-) So in this script I simply add a custom step where I install my dependencies with Poetry, and once it's done I trigger the collectstatic Django command. It's easy then to add other custom steps to this script, depending on the project. To get this work we only need to do these 2 things beforehand: Still have a root requirements.txt file - so that Heroku automatically recognises a Python project - and specify one single dependency there: Poetry itself. # file: /requirements.txt # This is only required to satisfy Heroku's Python build pack, # which doesn't handle Poetry (yet?). # The real dependencies install will happen in the \"bin/post_compile\" file. poetry == 1 .1.13 Ask the buildpack to not try to run collectstatic itself in its own compilation script: in production we serve our static assets with WhiteNoise , but this package is installed with Poetry later on in our bin/post_compile script and would not be found at the time when the builtin compilation script is triggered. Thankfully, the buildpack's documentation explains how to opt-out from this compilation step , simply by setting the DISABLE_COLLECTSTATIC environment variable. Which can be made via the Web dashboard or the Heroku CLI: heroku config:set DISABLE_COLLECTSTATIC = 1","title":"My bin/post_compile script"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#limitations-of-the-heroku-free-plan","text":"As every free plan, Heroku's one come with some limitations. The Postgres database is limited to 10,000 max rows, 1GB max storage and 20 connections at a time (details here ). For this \"Gin Rummy leaderboard\" these limits are not a problem The app itself is put into \"sleep\" after 30 minutes of inactivity, and waking it up typically takes a few seconds. That's obviously a very annoying restriction for \"real\" projects However, in this case this is just a simple Web page that will only record the Gin Rummy scores of the games I play with my partner, so having to wait a few sec for recording the first score when we start to play a few rounds is not a big deal. With real commercial projects I tend to start with the free plan, so project managers can start using the project as soon as possible - and we migrate to paid plans only when we need to.","title":"Limitations of the Heroku free plan"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#using-the-app","text":"By picking Heroku, a platform I'm familiar with, I was able to have this \"Gin Rummy leaderboard\" Django app online in production around 5pm; we started to record our first scores there an hour later when we met at the pub. I typed django-admin startproject at 9am, and used the project in production at 6pm - challenge accomplished! ...and it was really fun to give myself this constraint","title":"Using the app"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#addendum-deployment-to-flyio","text":"Out of curiosity, a few days later I also implemented the deployment of the Django project to Fly.io - which is a platform I wanted to try for a while. I just had to create a Dockerfile for my app, and then follow their documentation to deploy my Docker image to their infrastructure and link a Postgres database to it. Despite the fact that it was the very first time I was using it, one hour later I had my instance running - which tells how straightforward deploying an app to Fly.io is! My quick fly.toml file looks like this: app = \"[the name of my app]\" kill_signal = \"SIGINT\" kill_timeout = 5 processes = [] [env] PORT = \"8080\" DJANGO_SETTINGS_MODULE = \"project.settings.flyio\" [[services]] internal_port = 8080 processes = [ \"app\" ] protocol = \"tcp\" script_checks = [] [[services.ports]] force_https = true handlers = [ \"http\" ] port = 80 [[services.ports]] handlers = [ \"tls\" , \"http\" ] port = 443 [[services.http_checks]] interval = 15000 grace_period = \"5s\" method = \"get\" path = \"/\" protocol = \"https\" restart_limit = 0 timeout = 1000 tls_skip_verify = false [[statics]] guest_path = \"/app/staticfiles\" url_prefix = \"/static/\" And contrary to Heroku, the app doesn't sleep after 30 minutes of inactivity. The only downside is that although I'm using their free allowance I had to give my credit card detail to Fly.io before being able to deploy my first app - which is not the case with Heroku.","title":"Addendum: deployment to Fly.io"},{"location":"2022/08-09---from-scratch-to-online-in-production-in-a-single-day-with-django-part-3/#my-quick-django-project-dockerfile","text":"There are plenty of resources on the Web to create fine-tuned Dockerfiles for a Django project, but here is the one I quickly set up to deploy this side project to Fly.io: ARG PYTHON_VERSION = 3 .10 FROM python:${PYTHON_VERSION} AS build ENV DEBIAN_FRONTEND = noninteractive ENV PYTHONDONTWRITEBYTECODE = 1 PYTHONUNBUFFERED = 1 RUN apt-get update && apt-get install -y \\ python3-pip \\ python3-venv \\ python3-dev \\ python3-setuptools \\ python3-wheel \\ libpq-dev \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN pip install poetry RUN mkdir -p /app WORKDIR /app RUN python -m venv .venv COPY pyproject.toml poetry.lock ./ RUN poetry install --no-dev FROM python:${PYTHON_VERSION}-slim AS run ENV DEBIAN_FRONTEND = noninteractive RUN apt-get update && apt-get install -y \\ libpq5 \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN mkdir -p /app WORKDIR /app RUN addgroup -gid 1001 webapp RUN useradd --gid 1001 --uid 1001 webapp RUN chown -R 1001 :1001 /app USER 1001:1001 COPY --chown = 1001 :1001 --from = build /app/.venv .venv COPY --chown = 1001 :1001 . . ENV PYTHONPATH = /app/src RUN SECRET_KEY = does-not-matter-for-this-command DATABASE_URL = sqlite://:memory: ALLOWED_HOSTS = fly.io \\ .venv/bin/python src/manage.py collectstatic --noinput EXPOSE 8080 CMD [ \".venv/bin/gunicorn\" , \"--bind\" , \":8080\" , \"--workers\" , \"2\" , \"project.wsgi\" ] There is of course room for a lot of improvement there, but this quick multi-stage Dockerfile creates a 220MB image which is not too bad","title":"My quick \"Django project\" Dockerfile"},{"location":"2022/08-22---porting-a-python-web-app-to-go/","tags":["golang","project layout"],"text":"Porting a Python web app to Go \u00b6 The other day I was describing in this post the structure I use for my Django backend, strongly inspired by HackSoft's styleguide . However, as a lot of design patterns the beauty of this one is that it's not specific to Django, and not even to Python; it can actually be used for pretty much any programming language and framework! Let's see how the \"Gin Rummy leaderboard\" I was describing in my 'From scratch to online in production' in a single day series can be ported to another language I love, Go ! \"Go\" or \"Golang\" ? Go is also often referred as \"Golang\": this term is mostly used when it comes to searching stuff on the Web related to Go - the name of this programming language was certainly not optimised for optimal results using only its 2 letters name File structure \u00b6 We can apply pretty much the same file structures than with the Django app, with only a slight adaptation to embrace the Go idioms - such as the package name internal to namespace the \"private\" code of the app / package. There is no \"standard\" files layout for Go, but one of the general principles recommended by its authors is to keep the directories structure as flat as possible - some Go apps/pakages even have their whole code in a single folder! Here is the whole content of my Go Web app: gin-scoring/ \u251c\u2500\u2500 cmd/ \u2502 \u251c\u2500\u2500 http_server/ \u2502 \u2502 \u2514\u2500\u2500 main.go \u2502 \u2514\u2500\u2500 quicktests/ \u2502 \u251c\u2500\u2500 get_hall_of_fame_global.go \u2502 \u251c\u2500\u2500 get_hall_of_fame_monthly.go \u2502 \u251c\u2500\u2500 get_last_games.go \u2502 \u2514\u2500\u2500 save_game_result.go \u251c\u2500\u2500 internal/ \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u251c\u2500\u2500 mutations/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 save_game_result.go \u2502 \u2502 \u251c\u2500\u2500 queries/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 calculate_hall_of_fame_global.go \u2502 \u2502 \u2502 \u251c\u2500\u2500 calculate_hall_of_fame_monthly.go \u2502 \u2502 \u2502 \u2514\u2500\u2500 get_last_games.go \u2502 \u2502 \u251c\u2500\u2500 gin_rummy.go \u2502 \u2502 \u2514\u2500\u2500 types.go \u2502 \u251c\u2500\u2500 http/ \u2502 \u2502 \u251c\u2500\u2500 templates/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 layouts/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.gohtml \u2502 \u2502 \u2502 \u2514\u2500\u2500 homepage.gohtml \u2502 \u2502 \u2514\u2500\u2500 handlers.go \u2502 \u251c\u2500\u2500 models/ # (1) \u2502 \u2502 \u251c\u2500\u2500 boil_queries.go \u2502 \u2502 \u251c\u2500\u2500 boil_table_names.go \u2502 \u2502 \u251c\u2500\u2500 boil_types.go \u2502 \u2502 \u251c\u2500\u2500 boil_view_names.go \u2502 \u2502 \u251c\u2500\u2500 game_result.go \u2502 \u2502 \u2514\u2500\u2500 psql_upsert.go \u2502 \u251c\u2500\u2500 config.go \u2502 \u2514\u2500\u2500 db.go \u251c\u2500\u2500 db_schema.sql \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 go.mod \u251c\u2500\u2500 go.sum \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 sqlboiler.toml The content of this folder is not versioned, as it's code generated by SQLBoiler (file tree generated as usual with tree --dirsfirst -F . - see tree 's MAN page) The main components of that Go app, in a nutshell \u00b6 Database schema management \u00b6 For this Go port of my Django app I opted for the tool sqldef to manage my database migrations. In a nutshell, I just have to describe my schema in a plain SQL file, and then run psqldef (because I'm using Postgres) to apply the schema updates to my database. Very handy - especially for such a small project! -- file: /db_schema.sql create table if not exists game_result ( id integer primary key generated by default as identity , player_north_name varchar not null , player_south_name varchar not null , outcome varchar not null , winner_name varchar , deadwood_value smallint not null , winner_score smallint , created_at timestamp not null ); create index on game_result ( created_at ); ORM \u00b6 On the ORM -ish side of things, I chose SQLBoiler . It will introspect the schema of my database, and generate strongly typed Go code that allows me to create, read, update and delete data As the generated code is quite verbose, for such a small project I chose to not version the package - running go generate can re-generate it when I need to. I also have a Make target that simply runs sqlboiler psql to do the same :-) # file: .gitignore /internal/models HTTP routing \u00b6 The HTTP layer is in the internal/http package, powered by the classic gorilla/mux HTTP router - although I could also simply have used Go's built-in URL routing engine , as it does the job pretty well too. Here is the HTTP entry point of the app, that initialises that stuff: // file: cmd/http_server/main.go package main import ( \"fmt\" apphttp \"github.com/olivierphi/gin-scoring/internal/http\" appinternal \"github.com/olivierphi/gin-scoring/internal\" \"github.com/gorilla/mux\" \"log\" \"net/http\" \"time\" ) func main () { err := apphttp . LoadTemplates () if err != nil { panic ( err ) } srv := & http . Server { Handler : createRouter (), Addr : appinternal . Config (). Addr , // (1) WriteTimeout : 15 * time . Second , ReadTimeout : 15 * time . Second , } fmt . Printf ( \"Server starting on '%s'\\n\" , srv . Addr ) log . Fatal ( srv . ListenAndServe ()) } func createRouter () http . Handler { r := mux . NewRouter () r . HandleFunc ( \"/\" , apphttp . HomepageHandler ). Methods ( \"GET\" ) r . HandleFunc ( \"/game/result\" , apphttp . PostGameResultHandler ). Methods ( \"POST\" ) r . HandleFunc ( \"/ping\" , apphttp . PingHandler ) return r } Powered by Viper \"Controllers\" \u00b6 The \"HTTP Handlers\" of this app are pretty standard - that's the Go equivalent of what we would call \"Controllers\" in frameworks such as Ruby On Rails, Symfony or Laravel, or \"Views\" in Django: they receive an HTTP request, and are in charge or sending back an HTTP response . For example, here is the one that displays the main HTML page, with the current stats for the leaderboard: // in file: internal/http/handlers.go func HomepageHandler ( w http . ResponseWriter , r * http . Request ) { ctx := r . Context () t , ok := templates [ \"homepage.gohtml\" ] // (1) if ! ok { http . Error ( w , \"Could not load template\" , 500 ) return } db := internal . DB () lastGames , err := queries . GetLastGames ( ctx , db ) if err != nil { http . Error ( w , err . Error (), 500 ) return } hallOfFameGlobal , err := queries . CalculateHallOfFameGlobal ( ctx , db ) if err != nil { http . Error ( w , err . Error (), 500 ) return } hallOfFameMonthly , err := queries . CalculateHallOfFameMonthly ( ctx , db ) if err != nil { http . Error ( w , err . Error (), 500 ) return } templateData := make ( map [ string ] interface {}) templateData [ \"LastGames\" ] = lastGames templateData [ \"HallOfFameGlobal\" ] = hallOfFameGlobal templateData [ \"HallOfFameMonthly\" ] = hallOfFameMonthly if err := t . Execute ( w , templateData ); err != nil { http . Error ( w , fmt . Sprintf ( \"Error while rendering template: %v\" , err ), 500 ) return } } Yes, using short names (even reduced to one single character) for variables is quite idiomatic in Go The typical signature for a HTTP handler for example is to receive the Request as r and the ResponseWriter as w :-) HTML templates \u00b6 My HTML templates, powered by Go's built-in template engine , live in the internal/http/templates folder. These HTML templates are embedded in the generated executable, using Go's built-in embed machinery . Here are 2 articles that I found very useful to build this: \"HTML templating and inheritance\" , from the excellent book Let's Go by Alex Edwards: https://lets-go.alexedwards.net/sample/02.07-html-templating-and-inheritance.html \"Learn how to use the embed package in Go by building a web page easily\" : https://charly3pins.dev/blog/learn-how-to-use-the-embed-package-in-go-by-building-a-web-page-easily/ Quick tests, in lieu of a REPL \u00b6 In my cmd/quicktests folder I have Go files that I use for some CLI quick tests of my domain layer - the same way I would have used the REPL (powered by IPython ) of the Django shell on my Django app :-) Here is an example of such a file: // file: cmd/quicktests/get_hall_of_fame_monthly.go package main import ( \"context\" \"fmt\" \"github.com/olivierphi/gin-scoring/internal\" \"github.com/olivierphi/gin-scoring/internal/domain/queries\" ) func main () { ctx := context . Background () db := internal . DB () hallOfFameRows , err := queries . CalculateHallOfFameMonthly ( ctx , db ) if err != nil { panic ( err ) } fmt . Printf ( \"Found %d monthly hall of fame rows\\n\" , len ( hallOfFameRows )) for _ , row := range hallOfFameRows { fmt . Printf ( \"Row: %#v\\n\" , row ) } } To compile and execute such a file I just run the following command in the terminal: go run cmd/quicktests/get_hall_of_fame_monthly.go Implementing the business logic \u00b6 Last but not least, the business logic of the app lives in the internal/domain package - which really is a port of the Python/Django domain package , almost translated as-is in Go, using the same principles! The internal/domain package \u00b6 The logic is the same as for the Django app, that I described there : Is it code that creates, updates or deletes data in a database? Let's create a new Go file in the internal/domain/mutations package. Each of these files exposes one single function - its name will start with a verb -, and the input of this function is a struct that describes the data we need to execute that business action. Is it code that reads data from a database? Let's create a new Go file in the internal/domain/queries package. Is it code that expresses the business logic but neither alters nor reads data from a database? Let's put that in a new Go file of the internal/domain/ package. As they are interacting with the database, all the \"domain\" functions we create in the \"mutations\" and \"queries\" packages will always use the following 2 first arguments: ctx context.Context, db boil.ContextExecutor The first one is a classic Go context - detailed in this article for example. The second one is a SQLBoiler ContextExecutor - basically, an entry point to the API generated by SQLBoiler Ok, let's see some code snippets that illustrate this! Disclaimer: this is not 'production level' code The following Go code is pretty naive , just because this is a side project only used by my partner and me to track the results of our Gin Rummy games If that was a professional project some parts would be more \"industrial\" - the errors management or the data validation for example would be managed with more care. Disclaimer II: I'm not a seasoned Go developer Although I used Go to build some microservices in my professional coding activity, I still have way less experience with Go than I have with my \"core\" stacks (these days: Python, Django, TypeScript, React); consequently, the following code is likely not the best and most idiomatic Go code ever written These 2 disclaimers in mind, let's proceed with some examples of the code that powers this Go version of my \"Gin Rummy leaderboard\"... A \"mutation\" \u00b6 // file: internal/domain/mutations/save_game_result.go package mutations import ( \"context\" \"fmt\" \"strings\" \"github.com/volatiletech/null/v8\" \"github.com/volatiletech/sqlboiler/v4/boil\" \"github.com/olivierphi/gin-scoring/internal/domain\" \"github.com/olivierphi/gin-scoring/internal/models\" ) type SaveGameResultCommand struct { PlayerNorthName string PlayerSouthName string Outcome domain . GameOutcome WinnerName * string DeadwoodValue uint } func SaveGameResult ( ctx context . Context , db boil . ContextExecutor , cmd SaveGameResultCommand ) ( * models . GameResult , error ) { err := checkSaveGameResultInput ( cmd ) if err != nil { return nil , err } winnerScore , err := domain . CalculateRoundScore ( cmd . Outcome , cmd . DeadwoodValue ) if err != nil { return nil , err } playerNorthName := strings . ToLower ( cmd . PlayerNorthName ) playerSouthName := strings . ToLower ( cmd . PlayerSouthName ) var winnerNamePtr * string if cmd . WinnerName != nil { winnerName := strings . ToLower ( * cmd . WinnerName ) winnerNamePtr = & winnerName } resultModel := models . GameResult { // (1) PlayerNorthName : playerNorthName , PlayerSouthName : playerSouthName , Outcome : string ( cmd . Outcome ), DeadwoodValue : int16 ( cmd . DeadwoodValue ), WinnerName : null . StringFromPtr ( winnerNamePtr ), WinnerScore : null . Int16From ( int16 ( winnerScore )), } err = resultModel . Insert ( ctx , db , boil . Infer ()) if err != nil { return nil , err } return & resultModel , nil } func checkSaveGameResultInput ( cmd SaveGameResultCommand ) error { //TODO: use proper validation, powered by the \"validator\" package :-) // Check the outcome outcomeOk := false for _ , outcome := range domain . ValidGameOutcomes { if outcome == cmd . Outcome { outcomeOk = true break } } if ! outcomeOk { return fmt . Errorf ( \"invalid game outcome '%s'\" , cmd . Outcome ) } // Winner name must be one of the 2 players's name winnerName := * cmd . WinnerName if cmd . Outcome != domain . GameOutcomeDraw && winnerName != cmd . PlayerNorthName && winnerName != cmd . PlayerSouthName { return fmt . Errorf ( \"winner name '%s' is neither '%s' or '%s'\" , winnerName , cmd . PlayerNorthName , cmd . PlayerSouthName ) } return nil } models.GameResult is a strongly-typed struct generated from our database schema by SQLBoiler. Using wrong types for it will prevent the compilation of the Go program. Info On a \"real\" project I would have used the validator package to handle the data validation: https://pkg.go.dev/github.com/go-playground/validator/v10 A \"query\" \u00b6 // file: internal/domain/queries/calculate_hall_of_fame_monthly.go package queries import ( \"context\" \"sort\" \"time\" \"github.com/volatiletech/sqlboiler/v4/boil\" \"github.com/volatiletech/sqlboiler/v4/queries\" ) type hallOfFameMonthlyRowRaw struct { Month time . Time `boil:\"month\"` WinnerName string `boil:\"winner_name\"` WinCounts int `boil:\"win_counts\"` GrandTotal int `boil:\"grand_total\"` } type HallOfFameMonthlyRow struct { Month time . Time `boil:\"month\"` GameCounts int `boil:\"-\"` WinnerName string `boil:\"winner_name\"` WinCounts int `boil:\"win_counts\"` GrandTotal int `boil:\"grand_total\"` Delta int `boil:\"-\"` } const getHallOFameMonthlySQL = ` with first_pass as ( select date_trunc('month', created_at) as month, winner_name, count(*) as win_counts, sum(winner_score) as total_score from game_result where winner_score is not null group by winner_name, month order by month desc, win_counts desc ) select month, winner_name, win_counts, total_score, (total_score + (win_counts * 25)) as grand_total from first_pass order by grand_total desc ` func CalculateHallOfFameMonthly ( ctx context . Context , db boil . ContextExecutor ) ([] * HallOfFameMonthlyRow , error ) { var rawRes [] * hallOfFameMonthlyRowRaw err := queries . Raw ( getHallOFameMonthlySQL ). Bind ( ctx , db , & rawRes ) if err != nil { return nil , err } // Let's group our rows by month: resByMonth := make ( map [ time . Time ][] * hallOfFameMonthlyRowRaw ) for _ , rawRow := range rawRes { monthRows , ok := resByMonth [ rawRow . Month ] if ! ok { monthRows := [] * hallOfFameMonthlyRowRaw {} resByMonth [ rawRow . Month ] = monthRows } resByMonth [ rawRow . Month ] = append ( monthRows , & hallOfFameMonthlyRowRaw { Month : rawRow . Month , WinnerName : rawRow . WinnerName , WinCounts : rawRow . WinCounts , GrandTotal : rawRow . GrandTotal , }) } res := make ([] * HallOfFameMonthlyRow , 0 , len ( resByMonth )) for month , rawRows := range resByMonth { gameCounts := 0 for _ , rawRow := range rawRows { gameCounts += rawRow . WinCounts } winner := rawRows [ 0 ] winnerGrandTotal := winner . GrandTotal var delta int if len ( rawRows ) > 1 { secondBest := rawRows [ 1 ] delta = winnerGrandTotal - secondBest . GrandTotal } else { delta = winnerGrandTotal } res = append ( res , & HallOfFameMonthlyRow { Month : month , GameCounts : gameCounts , WinnerName : winner . WinnerName , WinCounts : winner . WinCounts , GrandTotal : winnerGrandTotal , Delta : delta , }) } sort . Slice ( res , func ( i , j int ) bool { return res [ i ]. Month . After ( res [ j ]. Month ) }) return res , nil } A \"pure domain\" function \u00b6 // file: internal/domain/gin_rummy.go package domain import \"fmt\" func CalculateRoundScore ( outcome GameOutcome , deadwood uint ) ( score uint , err error ) { switch outcome { case GameOutcomeDraw : return case GameOutcomeKnock : score = deadwood return case GameOutcomeGin : score = deadwood + 25 return case GameOutcomeBigGin : score = deadwood + 31 return case GameOutcomeUndercut : score = deadwood + 15 return } return 0 , fmt . Errorf ( \"invalid game outcome '%#v'\" , outcome ) } And that's it! \u00b6 Even though my day-to-day job mainly involves Python and Django - as well as some React and TypeScript here and there -, I like keeping some other skills \"warm\" - and Go certainly is one of them! Porting this \"Gin Rummy leaderboard\" Django app to Go took me around 8 hours, but it was totally worth it, as it allows me to keep a bit of my \"coding in Go\" muscle memory","title":"Porting a Python web app to Go"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#porting-a-python-web-app-to-go","text":"The other day I was describing in this post the structure I use for my Django backend, strongly inspired by HackSoft's styleguide . However, as a lot of design patterns the beauty of this one is that it's not specific to Django, and not even to Python; it can actually be used for pretty much any programming language and framework! Let's see how the \"Gin Rummy leaderboard\" I was describing in my 'From scratch to online in production' in a single day series can be ported to another language I love, Go ! \"Go\" or \"Golang\" ? Go is also often referred as \"Golang\": this term is mostly used when it comes to searching stuff on the Web related to Go - the name of this programming language was certainly not optimised for optimal results using only its 2 letters name","title":"Porting a Python web app to Go"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#file-structure","text":"We can apply pretty much the same file structures than with the Django app, with only a slight adaptation to embrace the Go idioms - such as the package name internal to namespace the \"private\" code of the app / package. There is no \"standard\" files layout for Go, but one of the general principles recommended by its authors is to keep the directories structure as flat as possible - some Go apps/pakages even have their whole code in a single folder! Here is the whole content of my Go Web app: gin-scoring/ \u251c\u2500\u2500 cmd/ \u2502 \u251c\u2500\u2500 http_server/ \u2502 \u2502 \u2514\u2500\u2500 main.go \u2502 \u2514\u2500\u2500 quicktests/ \u2502 \u251c\u2500\u2500 get_hall_of_fame_global.go \u2502 \u251c\u2500\u2500 get_hall_of_fame_monthly.go \u2502 \u251c\u2500\u2500 get_last_games.go \u2502 \u2514\u2500\u2500 save_game_result.go \u251c\u2500\u2500 internal/ \u2502 \u251c\u2500\u2500 domain/ \u2502 \u2502 \u251c\u2500\u2500 mutations/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 save_game_result.go \u2502 \u2502 \u251c\u2500\u2500 queries/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 calculate_hall_of_fame_global.go \u2502 \u2502 \u2502 \u251c\u2500\u2500 calculate_hall_of_fame_monthly.go \u2502 \u2502 \u2502 \u2514\u2500\u2500 get_last_games.go \u2502 \u2502 \u251c\u2500\u2500 gin_rummy.go \u2502 \u2502 \u2514\u2500\u2500 types.go \u2502 \u251c\u2500\u2500 http/ \u2502 \u2502 \u251c\u2500\u2500 templates/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 layouts/ \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.gohtml \u2502 \u2502 \u2502 \u2514\u2500\u2500 homepage.gohtml \u2502 \u2502 \u2514\u2500\u2500 handlers.go \u2502 \u251c\u2500\u2500 models/ # (1) \u2502 \u2502 \u251c\u2500\u2500 boil_queries.go \u2502 \u2502 \u251c\u2500\u2500 boil_table_names.go \u2502 \u2502 \u251c\u2500\u2500 boil_types.go \u2502 \u2502 \u251c\u2500\u2500 boil_view_names.go \u2502 \u2502 \u251c\u2500\u2500 game_result.go \u2502 \u2502 \u2514\u2500\u2500 psql_upsert.go \u2502 \u251c\u2500\u2500 config.go \u2502 \u2514\u2500\u2500 db.go \u251c\u2500\u2500 db_schema.sql \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 go.mod \u251c\u2500\u2500 go.sum \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 sqlboiler.toml The content of this folder is not versioned, as it's code generated by SQLBoiler (file tree generated as usual with tree --dirsfirst -F . - see tree 's MAN page)","title":"File structure"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#the-main-components-of-that-go-app-in-a-nutshell","text":"","title":"The main components of that Go app, in a nutshell"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#database-schema-management","text":"For this Go port of my Django app I opted for the tool sqldef to manage my database migrations. In a nutshell, I just have to describe my schema in a plain SQL file, and then run psqldef (because I'm using Postgres) to apply the schema updates to my database. Very handy - especially for such a small project! -- file: /db_schema.sql create table if not exists game_result ( id integer primary key generated by default as identity , player_north_name varchar not null , player_south_name varchar not null , outcome varchar not null , winner_name varchar , deadwood_value smallint not null , winner_score smallint , created_at timestamp not null ); create index on game_result ( created_at );","title":"Database schema management"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#orm","text":"On the ORM -ish side of things, I chose SQLBoiler . It will introspect the schema of my database, and generate strongly typed Go code that allows me to create, read, update and delete data As the generated code is quite verbose, for such a small project I chose to not version the package - running go generate can re-generate it when I need to. I also have a Make target that simply runs sqlboiler psql to do the same :-) # file: .gitignore /internal/models","title":"ORM"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#http-routing","text":"The HTTP layer is in the internal/http package, powered by the classic gorilla/mux HTTP router - although I could also simply have used Go's built-in URL routing engine , as it does the job pretty well too. Here is the HTTP entry point of the app, that initialises that stuff: // file: cmd/http_server/main.go package main import ( \"fmt\" apphttp \"github.com/olivierphi/gin-scoring/internal/http\" appinternal \"github.com/olivierphi/gin-scoring/internal\" \"github.com/gorilla/mux\" \"log\" \"net/http\" \"time\" ) func main () { err := apphttp . LoadTemplates () if err != nil { panic ( err ) } srv := & http . Server { Handler : createRouter (), Addr : appinternal . Config (). Addr , // (1) WriteTimeout : 15 * time . Second , ReadTimeout : 15 * time . Second , } fmt . Printf ( \"Server starting on '%s'\\n\" , srv . Addr ) log . Fatal ( srv . ListenAndServe ()) } func createRouter () http . Handler { r := mux . NewRouter () r . HandleFunc ( \"/\" , apphttp . HomepageHandler ). Methods ( \"GET\" ) r . HandleFunc ( \"/game/result\" , apphttp . PostGameResultHandler ). Methods ( \"POST\" ) r . HandleFunc ( \"/ping\" , apphttp . PingHandler ) return r } Powered by Viper","title":"HTTP routing"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#controllers","text":"The \"HTTP Handlers\" of this app are pretty standard - that's the Go equivalent of what we would call \"Controllers\" in frameworks such as Ruby On Rails, Symfony or Laravel, or \"Views\" in Django: they receive an HTTP request, and are in charge or sending back an HTTP response . For example, here is the one that displays the main HTML page, with the current stats for the leaderboard: // in file: internal/http/handlers.go func HomepageHandler ( w http . ResponseWriter , r * http . Request ) { ctx := r . Context () t , ok := templates [ \"homepage.gohtml\" ] // (1) if ! ok { http . Error ( w , \"Could not load template\" , 500 ) return } db := internal . DB () lastGames , err := queries . GetLastGames ( ctx , db ) if err != nil { http . Error ( w , err . Error (), 500 ) return } hallOfFameGlobal , err := queries . CalculateHallOfFameGlobal ( ctx , db ) if err != nil { http . Error ( w , err . Error (), 500 ) return } hallOfFameMonthly , err := queries . CalculateHallOfFameMonthly ( ctx , db ) if err != nil { http . Error ( w , err . Error (), 500 ) return } templateData := make ( map [ string ] interface {}) templateData [ \"LastGames\" ] = lastGames templateData [ \"HallOfFameGlobal\" ] = hallOfFameGlobal templateData [ \"HallOfFameMonthly\" ] = hallOfFameMonthly if err := t . Execute ( w , templateData ); err != nil { http . Error ( w , fmt . Sprintf ( \"Error while rendering template: %v\" , err ), 500 ) return } } Yes, using short names (even reduced to one single character) for variables is quite idiomatic in Go The typical signature for a HTTP handler for example is to receive the Request as r and the ResponseWriter as w :-)","title":"\"Controllers\""},{"location":"2022/08-22---porting-a-python-web-app-to-go/#html-templates","text":"My HTML templates, powered by Go's built-in template engine , live in the internal/http/templates folder. These HTML templates are embedded in the generated executable, using Go's built-in embed machinery . Here are 2 articles that I found very useful to build this: \"HTML templating and inheritance\" , from the excellent book Let's Go by Alex Edwards: https://lets-go.alexedwards.net/sample/02.07-html-templating-and-inheritance.html \"Learn how to use the embed package in Go by building a web page easily\" : https://charly3pins.dev/blog/learn-how-to-use-the-embed-package-in-go-by-building-a-web-page-easily/","title":"HTML templates"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#quick-tests-in-lieu-of-a-repl","text":"In my cmd/quicktests folder I have Go files that I use for some CLI quick tests of my domain layer - the same way I would have used the REPL (powered by IPython ) of the Django shell on my Django app :-) Here is an example of such a file: // file: cmd/quicktests/get_hall_of_fame_monthly.go package main import ( \"context\" \"fmt\" \"github.com/olivierphi/gin-scoring/internal\" \"github.com/olivierphi/gin-scoring/internal/domain/queries\" ) func main () { ctx := context . Background () db := internal . DB () hallOfFameRows , err := queries . CalculateHallOfFameMonthly ( ctx , db ) if err != nil { panic ( err ) } fmt . Printf ( \"Found %d monthly hall of fame rows\\n\" , len ( hallOfFameRows )) for _ , row := range hallOfFameRows { fmt . Printf ( \"Row: %#v\\n\" , row ) } } To compile and execute such a file I just run the following command in the terminal: go run cmd/quicktests/get_hall_of_fame_monthly.go","title":"Quick tests, in lieu of a REPL"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#implementing-the-business-logic","text":"Last but not least, the business logic of the app lives in the internal/domain package - which really is a port of the Python/Django domain package , almost translated as-is in Go, using the same principles!","title":"Implementing the business logic"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#the-internaldomain-package","text":"The logic is the same as for the Django app, that I described there : Is it code that creates, updates or deletes data in a database? Let's create a new Go file in the internal/domain/mutations package. Each of these files exposes one single function - its name will start with a verb -, and the input of this function is a struct that describes the data we need to execute that business action. Is it code that reads data from a database? Let's create a new Go file in the internal/domain/queries package. Is it code that expresses the business logic but neither alters nor reads data from a database? Let's put that in a new Go file of the internal/domain/ package. As they are interacting with the database, all the \"domain\" functions we create in the \"mutations\" and \"queries\" packages will always use the following 2 first arguments: ctx context.Context, db boil.ContextExecutor The first one is a classic Go context - detailed in this article for example. The second one is a SQLBoiler ContextExecutor - basically, an entry point to the API generated by SQLBoiler Ok, let's see some code snippets that illustrate this! Disclaimer: this is not 'production level' code The following Go code is pretty naive , just because this is a side project only used by my partner and me to track the results of our Gin Rummy games If that was a professional project some parts would be more \"industrial\" - the errors management or the data validation for example would be managed with more care. Disclaimer II: I'm not a seasoned Go developer Although I used Go to build some microservices in my professional coding activity, I still have way less experience with Go than I have with my \"core\" stacks (these days: Python, Django, TypeScript, React); consequently, the following code is likely not the best and most idiomatic Go code ever written These 2 disclaimers in mind, let's proceed with some examples of the code that powers this Go version of my \"Gin Rummy leaderboard\"...","title":"The internal/domain package"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#a-mutation","text":"// file: internal/domain/mutations/save_game_result.go package mutations import ( \"context\" \"fmt\" \"strings\" \"github.com/volatiletech/null/v8\" \"github.com/volatiletech/sqlboiler/v4/boil\" \"github.com/olivierphi/gin-scoring/internal/domain\" \"github.com/olivierphi/gin-scoring/internal/models\" ) type SaveGameResultCommand struct { PlayerNorthName string PlayerSouthName string Outcome domain . GameOutcome WinnerName * string DeadwoodValue uint } func SaveGameResult ( ctx context . Context , db boil . ContextExecutor , cmd SaveGameResultCommand ) ( * models . GameResult , error ) { err := checkSaveGameResultInput ( cmd ) if err != nil { return nil , err } winnerScore , err := domain . CalculateRoundScore ( cmd . Outcome , cmd . DeadwoodValue ) if err != nil { return nil , err } playerNorthName := strings . ToLower ( cmd . PlayerNorthName ) playerSouthName := strings . ToLower ( cmd . PlayerSouthName ) var winnerNamePtr * string if cmd . WinnerName != nil { winnerName := strings . ToLower ( * cmd . WinnerName ) winnerNamePtr = & winnerName } resultModel := models . GameResult { // (1) PlayerNorthName : playerNorthName , PlayerSouthName : playerSouthName , Outcome : string ( cmd . Outcome ), DeadwoodValue : int16 ( cmd . DeadwoodValue ), WinnerName : null . StringFromPtr ( winnerNamePtr ), WinnerScore : null . Int16From ( int16 ( winnerScore )), } err = resultModel . Insert ( ctx , db , boil . Infer ()) if err != nil { return nil , err } return & resultModel , nil } func checkSaveGameResultInput ( cmd SaveGameResultCommand ) error { //TODO: use proper validation, powered by the \"validator\" package :-) // Check the outcome outcomeOk := false for _ , outcome := range domain . ValidGameOutcomes { if outcome == cmd . Outcome { outcomeOk = true break } } if ! outcomeOk { return fmt . Errorf ( \"invalid game outcome '%s'\" , cmd . Outcome ) } // Winner name must be one of the 2 players's name winnerName := * cmd . WinnerName if cmd . Outcome != domain . GameOutcomeDraw && winnerName != cmd . PlayerNorthName && winnerName != cmd . PlayerSouthName { return fmt . Errorf ( \"winner name '%s' is neither '%s' or '%s'\" , winnerName , cmd . PlayerNorthName , cmd . PlayerSouthName ) } return nil } models.GameResult is a strongly-typed struct generated from our database schema by SQLBoiler. Using wrong types for it will prevent the compilation of the Go program. Info On a \"real\" project I would have used the validator package to handle the data validation: https://pkg.go.dev/github.com/go-playground/validator/v10","title":"A \"mutation\""},{"location":"2022/08-22---porting-a-python-web-app-to-go/#a-query","text":"// file: internal/domain/queries/calculate_hall_of_fame_monthly.go package queries import ( \"context\" \"sort\" \"time\" \"github.com/volatiletech/sqlboiler/v4/boil\" \"github.com/volatiletech/sqlboiler/v4/queries\" ) type hallOfFameMonthlyRowRaw struct { Month time . Time `boil:\"month\"` WinnerName string `boil:\"winner_name\"` WinCounts int `boil:\"win_counts\"` GrandTotal int `boil:\"grand_total\"` } type HallOfFameMonthlyRow struct { Month time . Time `boil:\"month\"` GameCounts int `boil:\"-\"` WinnerName string `boil:\"winner_name\"` WinCounts int `boil:\"win_counts\"` GrandTotal int `boil:\"grand_total\"` Delta int `boil:\"-\"` } const getHallOFameMonthlySQL = ` with first_pass as ( select date_trunc('month', created_at) as month, winner_name, count(*) as win_counts, sum(winner_score) as total_score from game_result where winner_score is not null group by winner_name, month order by month desc, win_counts desc ) select month, winner_name, win_counts, total_score, (total_score + (win_counts * 25)) as grand_total from first_pass order by grand_total desc ` func CalculateHallOfFameMonthly ( ctx context . Context , db boil . ContextExecutor ) ([] * HallOfFameMonthlyRow , error ) { var rawRes [] * hallOfFameMonthlyRowRaw err := queries . Raw ( getHallOFameMonthlySQL ). Bind ( ctx , db , & rawRes ) if err != nil { return nil , err } // Let's group our rows by month: resByMonth := make ( map [ time . Time ][] * hallOfFameMonthlyRowRaw ) for _ , rawRow := range rawRes { monthRows , ok := resByMonth [ rawRow . Month ] if ! ok { monthRows := [] * hallOfFameMonthlyRowRaw {} resByMonth [ rawRow . Month ] = monthRows } resByMonth [ rawRow . Month ] = append ( monthRows , & hallOfFameMonthlyRowRaw { Month : rawRow . Month , WinnerName : rawRow . WinnerName , WinCounts : rawRow . WinCounts , GrandTotal : rawRow . GrandTotal , }) } res := make ([] * HallOfFameMonthlyRow , 0 , len ( resByMonth )) for month , rawRows := range resByMonth { gameCounts := 0 for _ , rawRow := range rawRows { gameCounts += rawRow . WinCounts } winner := rawRows [ 0 ] winnerGrandTotal := winner . GrandTotal var delta int if len ( rawRows ) > 1 { secondBest := rawRows [ 1 ] delta = winnerGrandTotal - secondBest . GrandTotal } else { delta = winnerGrandTotal } res = append ( res , & HallOfFameMonthlyRow { Month : month , GameCounts : gameCounts , WinnerName : winner . WinnerName , WinCounts : winner . WinCounts , GrandTotal : winnerGrandTotal , Delta : delta , }) } sort . Slice ( res , func ( i , j int ) bool { return res [ i ]. Month . After ( res [ j ]. Month ) }) return res , nil }","title":"A \"query\""},{"location":"2022/08-22---porting-a-python-web-app-to-go/#a-pure-domain-function","text":"// file: internal/domain/gin_rummy.go package domain import \"fmt\" func CalculateRoundScore ( outcome GameOutcome , deadwood uint ) ( score uint , err error ) { switch outcome { case GameOutcomeDraw : return case GameOutcomeKnock : score = deadwood return case GameOutcomeGin : score = deadwood + 25 return case GameOutcomeBigGin : score = deadwood + 31 return case GameOutcomeUndercut : score = deadwood + 15 return } return 0 , fmt . Errorf ( \"invalid game outcome '%#v'\" , outcome ) }","title":"A \"pure domain\" function"},{"location":"2022/08-22---porting-a-python-web-app-to-go/#and-thats-it","text":"Even though my day-to-day job mainly involves Python and Django - as well as some React and TypeScript here and there -, I like keeping some other skills \"warm\" - and Go certainly is one of them! Porting this \"Gin Rummy leaderboard\" Django app to Go took me around 8 hours, but it was totally worth it, as it allows me to keep a bit of my \"coding in Go\" muscle memory","title":"And that's it!"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/","tags":["python","sqlalchemy","orm"],"text":"Using Alembic with SQLAlchemy 2 \u00b6 Abstract With a quick experiment I wanted to check if the current stable version of Alembic - the \"database migrations\" companion tool of the SQLAlchemy database abstraction layer and ORM - could be used with the brand new major version (still in Beta at the time of writing) of SQLAlchemy. As someone who cannot code without type annotations, I find it quite exciting to see that the Python ecosystem seems to be going more and more towards \"strongly type-hinted\" packages, the same way it's been happening in the JavaScript world with TypeScript during the last few years. The upcoming major version of SQlAlchemy - quite a major tool, with its 60 millions downloads per month ! - is bringing a very modern way to define Models, based on Python type hints. I mean... look at this beauty! from typing import Annotated , Optional from sqlalchemy import ForeignKey , String from sqlalchemy.orm import DeclarativeBase , Mapped , mapped_column , registry , relationship str50 = Annotated [ str , 50 ] class Base ( DeclarativeBase ): registry = registry ( type_annotation_map = { str50 : String ( 50 ), } ) # set up mapped_column() overrides, using whole column styles that are # expected to be used in multiple places intpk = Annotated [ int , mapped_column ( primary_key = True )] user_fk = Annotated [ int , mapped_column ( ForeignKey ( \"user_account.id\" ))] class User ( Base ): __tablename__ = \"user_account\" id : Mapped [ intpk ] # \u2728 name : Mapped [ str50 ] # \u2728\u2728 fullname : Mapped [ Optional [ str ]] # \u2728\u2728\u2728 addresses : Mapped [ list [ \"Address\" ]] = relationship ( # \u2728\u2728\u2728\u2728 back_populates = \"user\" ) class Address ( Base ): __tablename__ = \"address\" id : Mapped [ intpk ] email_address : Mapped [ str50 ] user_id : Mapped [ user_fk ] user : Mapped [ \"User\" ] = relationship ( back_populates = \"addresses\" ) This code comes from the very complete changelog of the 2.0.0b3 Beta release: https://docs.sqlalchemy.org/en/20/changelog/whatsnew_20.html Database migrations management \u00b6 As I can't live my backend developer life without a database migrations machinery (whether it's Django's excellent one, or before that Doctrine's, Rails', or Zend Framework's also great migrations systems), I cannot give a try to that version of SQLAlchemy without migrations. My issue was that even if I can start fiddling with SQLAlchemy 2 right away by installing the Beta release, there is nothing on its migration companion ( Alembic , also maintained by the folks behind SQLAlchemy) saying that it does work with this new version But who knows, maybe the \"metadata\" part of the SQLAlchemy API hasn't been changed so much with this 2.0 version - which would allow me to use the current stable version of Alembic with this shiny new release of SQLAlchemy? Let's give it a shot! Setting up a \"SQLAlchemy 2 + Alembic\" test project \u00b6 Common Python project setup \u00b6 As always, I'll start with a new virtual environment, with an up-to-date version of Pip and Poetry ready to serve: $ mkdir sqlalchemy2-test $ cd sqlalchemy2-test/ $ pyenv shell 3 .10.4 $ python -m venv .venv $ venv # my shell alias for `source .venv/bin/activate` ( .venv ) $ pip install -U pip poetry ( .venv ) $ poetry init Installing and configuring SQLAlchemy & Alembic \u00b6 Right, now let's install the Beta version of SQLAlchemy 2, as well as the stable version of Alembic: ( .venv ) $ poetry add SQLAlchemy == 2 .0.0b3 alembic Then, according to Alembic's tutorial we have to run an initalisation command: ( .venv ) $ alembic init alembic This command creates a alembic.ini at the root of my project, as well as an alembic/ folder at the same level, with some files pretty much ready to be used as is. For demonstration purpose I'll just create a models.py file at the root of my project, and copy-paste the nicely type-hinted User and Address models from the changelog in that file. sqlalchemy2-test/ \u251c\u2500\u2500 alembic/ \u2502 \u251c\u2500\u2500 versions/ \u2502 \u251c\u2500\u2500 env.py # Alembic Python config - using data from the INI file \u2502 \u251c\u2500\u2500 README \u2502 \u2514\u2500\u2500 script.py.mako # the template for generated migrations \u251c\u2500\u2500 alembic.ini # Alembic INI config \u251c\u2500\u2500 models.py # my SQLAlchemy models definition \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml The next step is to tell Alembic where the SQLAlchemy metadata are. This is done in the alembic/env.py file: # file: alembic/env.py # add your model's MetaData object here # for 'autogenerate' support from models import Base target_metadata = Base . metadata # and that's it! \ud83d\ude42 Hard-coded database URL in the INI file? Meh \ud83d\ude04 \u00b6 I could have left the alembic.ini and the alembic/env.py files as is (that's where Alembic gets its settings from), however I noticed that the URL of the database is hard-coded in the INI file - which is something I'm not a big fan of. # file: alembic.ini # N.B. The generated file is nicely documented \ud83d\udc4c, but for the sake of brevity # I'll strip the comments from this extract :-) [alembic] script_location = alembic prepend_sys_path = . sqlalchemy.url = driver://user:pass@localhost/dbname [loggers] ; etc In most modern Web projects the URL of the database we're interacting with comes from an environment variable (usually DATABASE_URL , but it can have any name really), so one can deploy their project on any number of environments and point to different databases on each of them, simply by setting the value of that variable. I really want to be able to use a DATABASE_URL environment variable, rather than relying on some hard-coded data in an INI file! But wait a minute... This alembic/env.py being a plain Python file , surely there must be a way for me to get the database URL injected from an environment variable, just by adding a tiny bit of Python logic? Using an DATABASE_URL environment variable to set up the database \u00b6 I'm not familiar at all with the content of that alembic/env.py file, but from what I understand of if after having skimmed through it... It seems that the content of the INI file - where the hard-coded database URL is - is used in 2 locations: # file: alembic/env.py ... # various stuff here def run_migrations_offline () -> None : url = config . get_main_option ( \"sqlalchemy.url\" ) context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online () -> None : connectable = engine_from_config ( config . get_section ( config . config_ini_section ), prefix = \"sqlalchemy.\" , poolclass = pool . NullPool , ) with connectable . connect () as connection : context . configure ( connection = connection , target_metadata = target_metadata ) with context . begin_transaction (): context . run_migrations () ... # various stuff there too Ok, so in one place it seems that we extract the database URL directly from the INI config ( url = config.get_main_option(\"sqlalchemy.url\") ), while in the other one we pass the whole config to SQLAlchemy ( config.get_section(config.config_ini_section) ). Let's see if I can inject the value of a DATABASE_URL environment variable to replace the hard-coded one, with the following changes (highlighted): # file: alembic/env.py import os SQLALCHEMY_URL = os . environ . get ( \"SQLALCHEMY_URL\" ) # And then, I can adapt the previous code: def run_migrations_offline () -> None : url = SQLALCHEMY_URL or config . get_main_option ( \"sqlalchemy.url\" ) context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online () -> None : ini_config = config . get_section ( config . config_ini_section ) if SQLALCHEMY_URL : ini_config [ \"sqlalchemy.url\" ] = SQLALCHEMY_URL connectable = engine_from_config ( ini_config , prefix = \"sqlalchemy.\" , poolclass = pool . NullPool , ) with connectable . connect () as connection : context . configure ( connection = connection , target_metadata = target_metadata ) with context . begin_transaction (): context . run_migrations () And now, let's try to generate an automatic database migration, from the User and Address models I've copy-pasted from the SQLAlchemy 2 changelog! Generating an automatic migration from the SQLAlchemy models \u00b6 ( .venv ) $ SQLALCHEMY_URL = sqlite+pysqlite:///db.sqlite3 ( .venv ) $ alembic revision --autogenerate -m \"add Users and Address\" Hey, it seems to work! Now I have a alembic/versions/694d8b4caa20_add_users_and_address.py file: \"\"\"add Users and Address Revision ID: 694d8b4caa20 Revises: Create Date: 2022-11-03 20:50:27.894030 \"\"\" import sqlalchemy as sa from alembic import op # revision identifiers, used by Alembic. revision = \"694d8b4caa20\" down_revision = None branch_labels = None depends_on = None def upgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . create_table ( \"user_account\" , sa . Column ( \"id\" , sa . Integer (), nullable = False ), sa . Column ( \"name\" , sa . String ( length = 30 ), nullable = False ), sa . Column ( \"fullname\" , sa . String (), nullable = True ), sa . PrimaryKeyConstraint ( \"id\" ), ) op . create_table ( \"address\" , sa . Column ( \"id\" , sa . Integer (), nullable = False ), sa . Column ( \"email_address\" , sa . String (), nullable = False ), sa . Column ( \"user_id\" , sa . Integer (), nullable = False ), sa . ForeignKeyConstraint ( [ \"user_id\" ], [ \"user_account.id\" ], ), sa . PrimaryKeyConstraint ( \"id\" ), ) # ### end Alembic commands ### def downgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . drop_table ( \"address\" ) op . drop_table ( \"user_account\" ) # ### end Alembic commands ### Applying the migrations \u00b6 Last but not least... Let's try to apply that database migration! ( .venv ) $ SQLALCHEMY_URL = sqlite+pysqlite:///db.sqlite3 ( .venv ) $ alembic upgrade head I can see that I now have a db.sqlite3 that doesn't seem empty... Let's open with a database editor: Bingo! Can I also revert the migration, and then re-apply it? ( .venv ) $ alembic alembic downgrade -1 # I can see in the database that the latest migration was indeed reverted! # Let's re-apply it: ( .venv ) $ alembic upgrade head # It works! \ud83d\ude42 Note If you want to have a look, the whole toy project (with an additional handy Makefile) can be found there: https://github.com/olivierphi/quicktest-alembic-sqlalchemy2/ Closing notes \u00b6 With this quick experiment I was able to check that the current stable version of Alembic can well and truly be used with the new major version of SQLAlchemy, which is neat. It's so nice to see major packages like SQLAlchemy opting for type-hinted code! As for me, I joined the world of Python because type hints were starting to be a thing there - so seeing things like that really please me! Exciting times!","title":"Using Alembic with SQLAlchemy 2"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#using-alembic-with-sqlalchemy-2","text":"Abstract With a quick experiment I wanted to check if the current stable version of Alembic - the \"database migrations\" companion tool of the SQLAlchemy database abstraction layer and ORM - could be used with the brand new major version (still in Beta at the time of writing) of SQLAlchemy. As someone who cannot code without type annotations, I find it quite exciting to see that the Python ecosystem seems to be going more and more towards \"strongly type-hinted\" packages, the same way it's been happening in the JavaScript world with TypeScript during the last few years. The upcoming major version of SQlAlchemy - quite a major tool, with its 60 millions downloads per month ! - is bringing a very modern way to define Models, based on Python type hints. I mean... look at this beauty! from typing import Annotated , Optional from sqlalchemy import ForeignKey , String from sqlalchemy.orm import DeclarativeBase , Mapped , mapped_column , registry , relationship str50 = Annotated [ str , 50 ] class Base ( DeclarativeBase ): registry = registry ( type_annotation_map = { str50 : String ( 50 ), } ) # set up mapped_column() overrides, using whole column styles that are # expected to be used in multiple places intpk = Annotated [ int , mapped_column ( primary_key = True )] user_fk = Annotated [ int , mapped_column ( ForeignKey ( \"user_account.id\" ))] class User ( Base ): __tablename__ = \"user_account\" id : Mapped [ intpk ] # \u2728 name : Mapped [ str50 ] # \u2728\u2728 fullname : Mapped [ Optional [ str ]] # \u2728\u2728\u2728 addresses : Mapped [ list [ \"Address\" ]] = relationship ( # \u2728\u2728\u2728\u2728 back_populates = \"user\" ) class Address ( Base ): __tablename__ = \"address\" id : Mapped [ intpk ] email_address : Mapped [ str50 ] user_id : Mapped [ user_fk ] user : Mapped [ \"User\" ] = relationship ( back_populates = \"addresses\" ) This code comes from the very complete changelog of the 2.0.0b3 Beta release: https://docs.sqlalchemy.org/en/20/changelog/whatsnew_20.html","title":"Using Alembic with SQLAlchemy 2"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#database-migrations-management","text":"As I can't live my backend developer life without a database migrations machinery (whether it's Django's excellent one, or before that Doctrine's, Rails', or Zend Framework's also great migrations systems), I cannot give a try to that version of SQLAlchemy without migrations. My issue was that even if I can start fiddling with SQLAlchemy 2 right away by installing the Beta release, there is nothing on its migration companion ( Alembic , also maintained by the folks behind SQLAlchemy) saying that it does work with this new version But who knows, maybe the \"metadata\" part of the SQLAlchemy API hasn't been changed so much with this 2.0 version - which would allow me to use the current stable version of Alembic with this shiny new release of SQLAlchemy? Let's give it a shot!","title":"Database migrations management"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#setting-up-a-sqlalchemy-2-alembic-test-project","text":"","title":"Setting up a \"SQLAlchemy 2 + Alembic\" test project"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#common-python-project-setup","text":"As always, I'll start with a new virtual environment, with an up-to-date version of Pip and Poetry ready to serve: $ mkdir sqlalchemy2-test $ cd sqlalchemy2-test/ $ pyenv shell 3 .10.4 $ python -m venv .venv $ venv # my shell alias for `source .venv/bin/activate` ( .venv ) $ pip install -U pip poetry ( .venv ) $ poetry init","title":"Common Python project setup"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#installing-and-configuring-sqlalchemy-alembic","text":"Right, now let's install the Beta version of SQLAlchemy 2, as well as the stable version of Alembic: ( .venv ) $ poetry add SQLAlchemy == 2 .0.0b3 alembic Then, according to Alembic's tutorial we have to run an initalisation command: ( .venv ) $ alembic init alembic This command creates a alembic.ini at the root of my project, as well as an alembic/ folder at the same level, with some files pretty much ready to be used as is. For demonstration purpose I'll just create a models.py file at the root of my project, and copy-paste the nicely type-hinted User and Address models from the changelog in that file. sqlalchemy2-test/ \u251c\u2500\u2500 alembic/ \u2502 \u251c\u2500\u2500 versions/ \u2502 \u251c\u2500\u2500 env.py # Alembic Python config - using data from the INI file \u2502 \u251c\u2500\u2500 README \u2502 \u2514\u2500\u2500 script.py.mako # the template for generated migrations \u251c\u2500\u2500 alembic.ini # Alembic INI config \u251c\u2500\u2500 models.py # my SQLAlchemy models definition \u251c\u2500\u2500 poetry.lock \u2514\u2500\u2500 pyproject.toml The next step is to tell Alembic where the SQLAlchemy metadata are. This is done in the alembic/env.py file: # file: alembic/env.py # add your model's MetaData object here # for 'autogenerate' support from models import Base target_metadata = Base . metadata # and that's it! \ud83d\ude42","title":"Installing and configuring SQLAlchemy &amp; Alembic"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#hard-coded-database-url-in-the-ini-file-meh","text":"I could have left the alembic.ini and the alembic/env.py files as is (that's where Alembic gets its settings from), however I noticed that the URL of the database is hard-coded in the INI file - which is something I'm not a big fan of. # file: alembic.ini # N.B. The generated file is nicely documented \ud83d\udc4c, but for the sake of brevity # I'll strip the comments from this extract :-) [alembic] script_location = alembic prepend_sys_path = . sqlalchemy.url = driver://user:pass@localhost/dbname [loggers] ; etc In most modern Web projects the URL of the database we're interacting with comes from an environment variable (usually DATABASE_URL , but it can have any name really), so one can deploy their project on any number of environments and point to different databases on each of them, simply by setting the value of that variable. I really want to be able to use a DATABASE_URL environment variable, rather than relying on some hard-coded data in an INI file! But wait a minute... This alembic/env.py being a plain Python file , surely there must be a way for me to get the database URL injected from an environment variable, just by adding a tiny bit of Python logic?","title":"Hard-coded database URL in the INI file? Meh \ud83d\ude04"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#using-an-database_url-environment-variable-to-set-up-the-database","text":"I'm not familiar at all with the content of that alembic/env.py file, but from what I understand of if after having skimmed through it... It seems that the content of the INI file - where the hard-coded database URL is - is used in 2 locations: # file: alembic/env.py ... # various stuff here def run_migrations_offline () -> None : url = config . get_main_option ( \"sqlalchemy.url\" ) context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online () -> None : connectable = engine_from_config ( config . get_section ( config . config_ini_section ), prefix = \"sqlalchemy.\" , poolclass = pool . NullPool , ) with connectable . connect () as connection : context . configure ( connection = connection , target_metadata = target_metadata ) with context . begin_transaction (): context . run_migrations () ... # various stuff there too Ok, so in one place it seems that we extract the database URL directly from the INI config ( url = config.get_main_option(\"sqlalchemy.url\") ), while in the other one we pass the whole config to SQLAlchemy ( config.get_section(config.config_ini_section) ). Let's see if I can inject the value of a DATABASE_URL environment variable to replace the hard-coded one, with the following changes (highlighted): # file: alembic/env.py import os SQLALCHEMY_URL = os . environ . get ( \"SQLALCHEMY_URL\" ) # And then, I can adapt the previous code: def run_migrations_offline () -> None : url = SQLALCHEMY_URL or config . get_main_option ( \"sqlalchemy.url\" ) context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online () -> None : ini_config = config . get_section ( config . config_ini_section ) if SQLALCHEMY_URL : ini_config [ \"sqlalchemy.url\" ] = SQLALCHEMY_URL connectable = engine_from_config ( ini_config , prefix = \"sqlalchemy.\" , poolclass = pool . NullPool , ) with connectable . connect () as connection : context . configure ( connection = connection , target_metadata = target_metadata ) with context . begin_transaction (): context . run_migrations () And now, let's try to generate an automatic database migration, from the User and Address models I've copy-pasted from the SQLAlchemy 2 changelog!","title":"Using an DATABASE_URL environment variable to set up the database"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#generating-an-automatic-migration-from-the-sqlalchemy-models","text":"( .venv ) $ SQLALCHEMY_URL = sqlite+pysqlite:///db.sqlite3 ( .venv ) $ alembic revision --autogenerate -m \"add Users and Address\" Hey, it seems to work! Now I have a alembic/versions/694d8b4caa20_add_users_and_address.py file: \"\"\"add Users and Address Revision ID: 694d8b4caa20 Revises: Create Date: 2022-11-03 20:50:27.894030 \"\"\" import sqlalchemy as sa from alembic import op # revision identifiers, used by Alembic. revision = \"694d8b4caa20\" down_revision = None branch_labels = None depends_on = None def upgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . create_table ( \"user_account\" , sa . Column ( \"id\" , sa . Integer (), nullable = False ), sa . Column ( \"name\" , sa . String ( length = 30 ), nullable = False ), sa . Column ( \"fullname\" , sa . String (), nullable = True ), sa . PrimaryKeyConstraint ( \"id\" ), ) op . create_table ( \"address\" , sa . Column ( \"id\" , sa . Integer (), nullable = False ), sa . Column ( \"email_address\" , sa . String (), nullable = False ), sa . Column ( \"user_id\" , sa . Integer (), nullable = False ), sa . ForeignKeyConstraint ( [ \"user_id\" ], [ \"user_account.id\" ], ), sa . PrimaryKeyConstraint ( \"id\" ), ) # ### end Alembic commands ### def downgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . drop_table ( \"address\" ) op . drop_table ( \"user_account\" ) # ### end Alembic commands ###","title":"Generating an automatic migration from the SQLAlchemy models"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#applying-the-migrations","text":"Last but not least... Let's try to apply that database migration! ( .venv ) $ SQLALCHEMY_URL = sqlite+pysqlite:///db.sqlite3 ( .venv ) $ alembic upgrade head I can see that I now have a db.sqlite3 that doesn't seem empty... Let's open with a database editor: Bingo! Can I also revert the migration, and then re-apply it? ( .venv ) $ alembic alembic downgrade -1 # I can see in the database that the latest migration was indeed reverted! # Let's re-apply it: ( .venv ) $ alembic upgrade head # It works! \ud83d\ude42 Note If you want to have a look, the whole toy project (with an additional handy Makefile) can be found there: https://github.com/olivierphi/quicktest-alembic-sqlalchemy2/","title":"Applying the migrations"},{"location":"2022/11-11---using-alembic-with-with-sqlalchemy-2/#closing-notes","text":"With this quick experiment I was able to check that the current stable version of Alembic can well and truly be used with the new major version of SQLAlchemy, which is neat. It's so nice to see major packages like SQLAlchemy opting for type-hinted code! As for me, I joined the world of Python because type hints were starting to be a thing there - so seeing things like that really please me! Exciting times!","title":"Closing notes"},{"location":"2023/05-17---til-content-hash-in-the-form-of-a-uuid/","tags":["til","python"],"text":"TIL : Hashing data in the form of a UUID \u00b6 A quick trick I learned today as I was working on a codebase related to the awesome Wagtail CMS . When we need to get a unique digest of some data - that will be the same over time if we apply the same function to the same data later on, while hiding what the original data was - we usually look at the classic hash functions such as SHA-1, HMAC and such... However, it turns out that we can also use UUIDs for such a purpose - which gives us a piece of data that is both a digest and an UUID, which can be quite handy! The original trick is there, in the Wagtail code: (note that it might be a trick that I was aware of but that that is commonly used elsewhere, of course) https://github.com/wagtail/wagtail/blob/85c9b66/wagtail/models/reference_index.py#L388-L404 Quick check \u00b6 We can check that in a quick Python shell: import uuid # Generated previously with `uuid.uuid4()` # --> we can now hard-code it in our code to get consistent hashes from here: hash_base_uuid = uuid . UUID ( \"e1fb6cc4-4843-4433-91a7-f7639640cb8d\" ) # Let's create some dummy data we want to create hashes for: data_to_hash_1 , data_to_hash_2 = \"little bobby\" , \"tables\" hash_1_round_1 = uuid . uuid5 ( hash_base_uuid , data_to_hash_1 ) # --> hashing \"little bobby\" with this base UUID will *always* # gives us `UUID('218fde61-5b70-5a33-9e0a-daa7f2a7c388')` # Hashing the same data again: hash_1_round_2 = uuid . uuid5 ( hash_base_uuid , data_to_hash_1 ) # Hashing the same data does give us the same digest-as-a-UUID: \u2705 assert hash_1_round_1 == hash_1_round_2 # Now hashing another bit of data hash_2_round_1 = uuid . uuid5 ( hash_base_uuid , data_to_hash_2 ) # --> hashing \"tables\" with this base UUID will *always* # gives us `UUID('189a5a73-6e52-5885-ba68-8432f5632e2d')` # Hashing different data does give us different digests-as-a-UUID: \u2705 assert hash_1_round_1 != hash_2_round_1 Potential vulnerability \u00b6 According to the Python documentation of the uuid module, it's a SHA-1 hash that will be used behind the scenes. This is way better than the other algorithm that can also be used according to RFC 4122 , which is the much weaker MD5. However, SHA-1 seems to be potentially vulnerable to some attacks nowadays, so I reckon it might not be something to use when the hashed data is susceptible to be attacked by brute-force? (user passwords, etc.) https://en.wikipedia.org/wiki/SHA-1#Attacks","title":"TIL: Hashing data in the form of a UUID"},{"location":"2023/05-17---til-content-hash-in-the-form-of-a-uuid/#til-hashing-data-in-the-form-of-a-uuid","text":"A quick trick I learned today as I was working on a codebase related to the awesome Wagtail CMS . When we need to get a unique digest of some data - that will be the same over time if we apply the same function to the same data later on, while hiding what the original data was - we usually look at the classic hash functions such as SHA-1, HMAC and such... However, it turns out that we can also use UUIDs for such a purpose - which gives us a piece of data that is both a digest and an UUID, which can be quite handy! The original trick is there, in the Wagtail code: (note that it might be a trick that I was aware of but that that is commonly used elsewhere, of course) https://github.com/wagtail/wagtail/blob/85c9b66/wagtail/models/reference_index.py#L388-L404","title":"TIL: Hashing data in the form of a UUID"},{"location":"2023/05-17---til-content-hash-in-the-form-of-a-uuid/#quick-check","text":"We can check that in a quick Python shell: import uuid # Generated previously with `uuid.uuid4()` # --> we can now hard-code it in our code to get consistent hashes from here: hash_base_uuid = uuid . UUID ( \"e1fb6cc4-4843-4433-91a7-f7639640cb8d\" ) # Let's create some dummy data we want to create hashes for: data_to_hash_1 , data_to_hash_2 = \"little bobby\" , \"tables\" hash_1_round_1 = uuid . uuid5 ( hash_base_uuid , data_to_hash_1 ) # --> hashing \"little bobby\" with this base UUID will *always* # gives us `UUID('218fde61-5b70-5a33-9e0a-daa7f2a7c388')` # Hashing the same data again: hash_1_round_2 = uuid . uuid5 ( hash_base_uuid , data_to_hash_1 ) # Hashing the same data does give us the same digest-as-a-UUID: \u2705 assert hash_1_round_1 == hash_1_round_2 # Now hashing another bit of data hash_2_round_1 = uuid . uuid5 ( hash_base_uuid , data_to_hash_2 ) # --> hashing \"tables\" with this base UUID will *always* # gives us `UUID('189a5a73-6e52-5885-ba68-8432f5632e2d')` # Hashing different data does give us different digests-as-a-UUID: \u2705 assert hash_1_round_1 != hash_2_round_1","title":"Quick check"},{"location":"2023/05-17---til-content-hash-in-the-form-of-a-uuid/#potential-vulnerability","text":"According to the Python documentation of the uuid module, it's a SHA-1 hash that will be used behind the scenes. This is way better than the other algorithm that can also be used according to RFC 4122 , which is the much weaker MD5. However, SHA-1 seems to be potentially vulnerable to some attacks nowadays, so I reckon it might not be something to use when the hashed data is susceptible to be attacked by brute-force? (user passwords, etc.) https://en.wikipedia.org/wiki/SHA-1#Attacks","title":"Potential vulnerability"},{"location":"2023/05-26---let-s-try-kitty-layouts/","tags":["terminal"],"text":"Let's try Kitty layouts \u00b6 A terminal multiplexer... \u00b6 As I spend a lot of my time in the terminal emulator, I find it useful to be able to display several shell sessions at once in the same window - so I can have one session running my backend app and one running the frontend one for example. In order to do this I've been using the terminal multiplexer tmux for several years, and I've been quite happy with it. ...And a terminal emulator... \u00b6 However, I've also been using the awesome terminal emulator Kitty for a while, and I have great respect for its author as they're also the developer behind the open source e-books management software Calibre - which is, like Kitty, a fine-tuned mix of C and Python code And in Kitty's FAQ they say the following: terminal multiplexers are a bad idea, do not use them, if at all possible. kitty contains features that do all of what tmux does, but better Alright, let's trust them and give Kitty's layouts a try then! ...Let's drop tmux, and just use Kitty's layouts! \u00b6 After having fiddled for a while with Kitty's config, here is the setup I ended up with: # ~/.config/kitty/kitty.conf # My default layout will be \"splits\". # (i.e. on-demand tmux-like panes, \"windows\" in Kitty's terminology) # But I also want to be able to switch to \"stack\" in order to temporarily # render the active window in full screen within Kitty: enabled_layouts splits,stack # With \"f1\" I can toggle between the two layouts: # i.e. with \"f1\" the windows I'm working in goes full screen, and pressing # \"f1\" again brings back the other windows. map f1 toggle_layout stack # With \"f5\" I can create a new window splitting the space used by the # existing one, so that the two windows are placed one above the other: map f5 launch --cwd =current --location=hsplit # With \"f6\" I can create a new window splitting the space used by the # existing one, so that the two windows are placed side by side: map f6 launch --cwd =current --location=vsplit Main Kitty documentation pages I've used: https://sw.kovidgoyal.net/kitty/layouts/ https://sw.kovidgoyal.net/kitty/conf/ Using only Kitty feels faster indeed, in term of how the terminal emulator's UI reacts. Now, I \"just\" (might take a while ) have to replace my previous mental map of keyboard shortcuts with the new one, and I should be good to go!","title":"Let's try Kitty layouts"},{"location":"2023/05-26---let-s-try-kitty-layouts/#lets-try-kitty-layouts","text":"","title":"Let's try Kitty layouts"},{"location":"2023/05-26---let-s-try-kitty-layouts/#a-terminal-multiplexer","text":"As I spend a lot of my time in the terminal emulator, I find it useful to be able to display several shell sessions at once in the same window - so I can have one session running my backend app and one running the frontend one for example. In order to do this I've been using the terminal multiplexer tmux for several years, and I've been quite happy with it.","title":"A terminal multiplexer..."},{"location":"2023/05-26---let-s-try-kitty-layouts/#and-a-terminal-emulator","text":"However, I've also been using the awesome terminal emulator Kitty for a while, and I have great respect for its author as they're also the developer behind the open source e-books management software Calibre - which is, like Kitty, a fine-tuned mix of C and Python code And in Kitty's FAQ they say the following: terminal multiplexers are a bad idea, do not use them, if at all possible. kitty contains features that do all of what tmux does, but better Alright, let's trust them and give Kitty's layouts a try then!","title":"...And a terminal emulator..."},{"location":"2023/05-26---let-s-try-kitty-layouts/#lets-drop-tmux-and-just-use-kittys-layouts","text":"After having fiddled for a while with Kitty's config, here is the setup I ended up with: # ~/.config/kitty/kitty.conf # My default layout will be \"splits\". # (i.e. on-demand tmux-like panes, \"windows\" in Kitty's terminology) # But I also want to be able to switch to \"stack\" in order to temporarily # render the active window in full screen within Kitty: enabled_layouts splits,stack # With \"f1\" I can toggle between the two layouts: # i.e. with \"f1\" the windows I'm working in goes full screen, and pressing # \"f1\" again brings back the other windows. map f1 toggle_layout stack # With \"f5\" I can create a new window splitting the space used by the # existing one, so that the two windows are placed one above the other: map f5 launch --cwd =current --location=hsplit # With \"f6\" I can create a new window splitting the space used by the # existing one, so that the two windows are placed side by side: map f6 launch --cwd =current --location=vsplit Main Kitty documentation pages I've used: https://sw.kovidgoyal.net/kitty/layouts/ https://sw.kovidgoyal.net/kitty/conf/ Using only Kitty feels faster indeed, in term of how the terminal emulator's UI reacts. Now, I \"just\" (might take a while ) have to replace my previous mental map of keyboard shortcuts with the new one, and I should be good to go!","title":"...Let's drop tmux, and just use Kitty's layouts!"},{"location":"2023/08-18---business_logic.py/","tags":["python","django"],"text":"Implementing the business logic in a Django project \u00b6 The \"startup project\" way \u00b6 In a previous post I explained how I usually manage business logic in a Django \"startup\" project, in an approximation of the CQS design pattern: How I manage business logic in a Django project I still love this way of organising the code, and how it allows one to fully focus on writing and maintaining the code , rather than wondering where to put this or that piece of code. The importance of having a guideline to organise the code When building a platform mostly made of bespoke code tailored to implement the specific business logic of a \"startup\" project , in my personal experience writing the business logic is what takes the most effort for the development team - especially when using a \"batteries included\" framework such as Django, Laravel, Symfony or Rails. This is why having that sort of simple guidelines to organise that vast quantity of code and avoid wasting time with \"which code goes where\" considerations is crucial in my opinion. The \"CMS-driven\" way \u00b6 However, in the last few months I've been working for a digital agency, creators of the Wagtail CMS, and I realised that in such projects the topics around business logic are quite different. When building a CMS-driven website, there is actually much less bespoke business logic to implement, since the project is heavily based on content management. And its is the CMS, and all its built-in features dedicated to content management workflow, permissions, notifications, etc, that really are the conductor of the project. Some of the Django apps will still require custom business logic in such a project, but it's less often the case than for a \"startup\" project. This is why I went for a simpler approach, where I put all the business logic of each Django app in a single Python module . Naming things \u00b6 As for the name of that module, I was going to opt for domain , but I thought back of this article and decided to call it business_logic instead, as it's really a name that leaves no room for ambiguity David Winterbottom, Why your models are fat Your web framework is not your boss. As a rule-of-thumb, your application logic should live in modules that aren\u2019t Django-specific modules (eg not in views.py , models.py or forms.py ). If I had my way, Django would create an empty business_logic.py in each new app to encourage this. https://codeinthehole.com/lists/why-your-models-are-fat/ What that file looks like \u00b6 Inside that file, I basically follow the same rules than in the CQS -inspired approach: The only classes are ones that describe data structures , and they are immutable every time it is possible. The classes I typically use or that are typing.NamedTuple , dataclasses.dataclass or typing.TypedDict . Choosing one or another depends on the specific use case ( typing.NamedTuple always being my own first choice \ud83d\udc9a), but they all pretty much address the same need anyhow The business logic is implemented in plain functions , and as soon as a function has more than one argument it must use the syntax that makes it a \"kwargs-only\" function. In a nutshell, it looks like this: # business_logic.py import enum from datetime import datetime from typing import NamedTuple , TypedDict from django.core.files import File from .models import Group # --- Constants EVENTBRITE_API_URL = \"https://www.eventbriteapi.com/v3/\" # --- Types class EventType ( enum . IntEnum ): CONFERENCE = 1 QUICK_TALK = 2 WORKSHOP = 3 class EventbriteNotificationParameters ( NamedTuple ): target_group : Group | None in_realtime : bool = False class EventbriteAPIEventData ( TypedDict ): \"\"\" Non-exhaustive description of the data returned by the Eventbrite API. \"\"\" id : str name : str description : str start : str end : str url : str logo : str # --- Queries def fetch_eventbrite_events_data ( * , from_date : datetime | None = None , only_in_location : str | None = None , ) -> list [ EventbriteAPIEventData ]: ... def fetch_eventbrite_event_data_by_id ( event_id : str ) -> EventbriteAPIEventData : ... # --- Mutations def create_eventbrite_event ( * , name : str , description : str , start : datetime , end : datetime , logo : File , ) -> EventbriteAPIEventData : ... Conclusion \u00b6 By following this very simple rule, every Django app that needs custom business logic has its own business_logic.py file, at the same level than its models.py and views.py ones. As always, it's better to start simple - some people even start their Django projects with a single folder after all! The idea is the same here: start with a single file, and then adapt by splitting this file into a package made of several modules only if the volume of code in that file makes it too tedious to work with.","title":"business_logic.py"},{"location":"2023/08-18---business_logic.py/#implementing-the-business-logic-in-a-django-project","text":"","title":"Implementing the business logic in a Django project"},{"location":"2023/08-18---business_logic.py/#the-startup-project-way","text":"In a previous post I explained how I usually manage business logic in a Django \"startup\" project, in an approximation of the CQS design pattern: How I manage business logic in a Django project I still love this way of organising the code, and how it allows one to fully focus on writing and maintaining the code , rather than wondering where to put this or that piece of code. The importance of having a guideline to organise the code When building a platform mostly made of bespoke code tailored to implement the specific business logic of a \"startup\" project , in my personal experience writing the business logic is what takes the most effort for the development team - especially when using a \"batteries included\" framework such as Django, Laravel, Symfony or Rails. This is why having that sort of simple guidelines to organise that vast quantity of code and avoid wasting time with \"which code goes where\" considerations is crucial in my opinion.","title":"The \"startup project\" way"},{"location":"2023/08-18---business_logic.py/#the-cms-driven-way","text":"However, in the last few months I've been working for a digital agency, creators of the Wagtail CMS, and I realised that in such projects the topics around business logic are quite different. When building a CMS-driven website, there is actually much less bespoke business logic to implement, since the project is heavily based on content management. And its is the CMS, and all its built-in features dedicated to content management workflow, permissions, notifications, etc, that really are the conductor of the project. Some of the Django apps will still require custom business logic in such a project, but it's less often the case than for a \"startup\" project. This is why I went for a simpler approach, where I put all the business logic of each Django app in a single Python module .","title":"The \"CMS-driven\" way"},{"location":"2023/08-18---business_logic.py/#naming-things","text":"As for the name of that module, I was going to opt for domain , but I thought back of this article and decided to call it business_logic instead, as it's really a name that leaves no room for ambiguity David Winterbottom, Why your models are fat Your web framework is not your boss. As a rule-of-thumb, your application logic should live in modules that aren\u2019t Django-specific modules (eg not in views.py , models.py or forms.py ). If I had my way, Django would create an empty business_logic.py in each new app to encourage this. https://codeinthehole.com/lists/why-your-models-are-fat/","title":"Naming things"},{"location":"2023/08-18---business_logic.py/#what-that-file-looks-like","text":"Inside that file, I basically follow the same rules than in the CQS -inspired approach: The only classes are ones that describe data structures , and they are immutable every time it is possible. The classes I typically use or that are typing.NamedTuple , dataclasses.dataclass or typing.TypedDict . Choosing one or another depends on the specific use case ( typing.NamedTuple always being my own first choice \ud83d\udc9a), but they all pretty much address the same need anyhow The business logic is implemented in plain functions , and as soon as a function has more than one argument it must use the syntax that makes it a \"kwargs-only\" function. In a nutshell, it looks like this: # business_logic.py import enum from datetime import datetime from typing import NamedTuple , TypedDict from django.core.files import File from .models import Group # --- Constants EVENTBRITE_API_URL = \"https://www.eventbriteapi.com/v3/\" # --- Types class EventType ( enum . IntEnum ): CONFERENCE = 1 QUICK_TALK = 2 WORKSHOP = 3 class EventbriteNotificationParameters ( NamedTuple ): target_group : Group | None in_realtime : bool = False class EventbriteAPIEventData ( TypedDict ): \"\"\" Non-exhaustive description of the data returned by the Eventbrite API. \"\"\" id : str name : str description : str start : str end : str url : str logo : str # --- Queries def fetch_eventbrite_events_data ( * , from_date : datetime | None = None , only_in_location : str | None = None , ) -> list [ EventbriteAPIEventData ]: ... def fetch_eventbrite_event_data_by_id ( event_id : str ) -> EventbriteAPIEventData : ... # --- Mutations def create_eventbrite_event ( * , name : str , description : str , start : datetime , end : datetime , logo : File , ) -> EventbriteAPIEventData : ...","title":"What that file looks like"},{"location":"2023/08-18---business_logic.py/#conclusion","text":"By following this very simple rule, every Django app that needs custom business logic has its own business_logic.py file, at the same level than its models.py and views.py ones. As always, it's better to start simple - some people even start their Django projects with a single folder after all! The idea is the same here: start with a single file, and then adapt by splitting this file into a package made of several modules only if the volume of code in that file makes it too tedious to work with.","title":"Conclusion"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: TIL \u00b6 Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment blog \u00b6 Building a Markdown-based blog deployment \u00b6 'From scratch to online in production' in a single day, with Django - Part 3 django \u00b6 Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment 'From scratch to online in production' in a single day, with Django - Part 3 business_logic.py github \u00b6 Triggering a GitHub Action from an external source golang \u00b6 Choosing a tech stack for my card game platform Porting a Python web app to Go laravel \u00b6 Choosing a tech stack for my card game platform mkdocs \u00b6 Building a Markdown-based blog next.js \u00b6 Choosing a tech stack for my card game platform orm \u00b6 Using Alembic with SQLAlchemy 2 project layout \u00b6 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Porting a Python web app to Go python \u00b6 Building a Markdown-based blog Using Alembic with SQLAlchemy 2 TIL: Hashing data in the form of a UUID business_logic.py rails \u00b6 Choosing a tech stack for my card game platform sqlalchemy \u00b6 Using Alembic with SQLAlchemy 2 sqlite \u00b6 Making SQLite much faster in a local dev environment terminal \u00b6 Let's try Kitty layouts til \u00b6 TIL: Hashing data in the form of a UUID","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#til","text":"Triggering a GitHub Action from an external source Making SQLite much faster in a local dev environment","title":"TIL"},{"location":"tags/#blog","text":"Building a Markdown-based blog","title":"blog"},{"location":"tags/#deployment","text":"'From scratch to online in production' in a single day, with Django - Part 3","title":"deployment"},{"location":"tags/#django","text":"Choosing a tech stack for my card game platform 'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Making SQLite much faster in a local dev environment 'From scratch to online in production' in a single day, with Django - Part 3 business_logic.py","title":"django"},{"location":"tags/#github","text":"Triggering a GitHub Action from an external source","title":"github"},{"location":"tags/#golang","text":"Choosing a tech stack for my card game platform Porting a Python web app to Go","title":"golang"},{"location":"tags/#laravel","text":"Choosing a tech stack for my card game platform","title":"laravel"},{"location":"tags/#mkdocs","text":"Building a Markdown-based blog","title":"mkdocs"},{"location":"tags/#nextjs","text":"Choosing a tech stack for my card game platform","title":"next.js"},{"location":"tags/#orm","text":"Using Alembic with SQLAlchemy 2","title":"orm"},{"location":"tags/#project-layout","text":"'From scratch to online in production' in a single day, with Django - Part 1 'From scratch to online in production' in a single day, with Django - Part 2 Porting a Python web app to Go","title":"project layout"},{"location":"tags/#python","text":"Building a Markdown-based blog Using Alembic with SQLAlchemy 2 TIL: Hashing data in the form of a UUID business_logic.py","title":"python"},{"location":"tags/#rails","text":"Choosing a tech stack for my card game platform","title":"rails"},{"location":"tags/#sqlalchemy","text":"Using Alembic with SQLAlchemy 2","title":"sqlalchemy"},{"location":"tags/#sqlite","text":"Making SQLite much faster in a local dev environment","title":"sqlite"},{"location":"tags/#terminal","text":"Let's try Kitty layouts","title":"terminal"},{"location":"tags/#til_1","text":"TIL: Hashing data in the form of a UUID","title":"til"}]}